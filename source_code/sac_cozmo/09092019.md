# Settimana 09-13/09/2019

## Problemi e soluzioni

### Problematica Policy Immobile

Durante i tentativi di training effettuati con l'algoritmo implementato, si è evidenziata una marcata differenza tra
training phase e testing phase. Le azioni del robot in fase di testing apparivano rallentate, ripetitive e, anche dopo
numerose epoche di addestramento, il robot continuava a comportarsi nello stesso modo.

Il primo tentativo effettuato è stato quello di aumentare le epoche di learning e manipolare la dimensione del batch
estratto ad ogni epoca. Questo non ha permesso di ottenere i risultati sperati: gli intervalli tra un episodio e l'altro
sono aumentati a causa dell'aumento dei parametri precedentemente indicati fornendo miglioramenti limitati.
Questo cambiamento avrebbe reso impossibile anche futuri esperimenti a causa dei tempi molto lunghi.

Per questo motivo si è affrontata una revisione del precedente codice utilizzato per risolvere positivamente il problema
fornito dall'environment di OpenAI Gym `Pendulum-v0`. In questo contesto, la dimensione dell'immagine di
stato del sistema presentava una risoluzione risoluzione molto ridotta rispetto a quella utilizzata nell'esperimento 
con Cozmo.

La decisione è stata quindi quella di ridurre la dimensione dell'immagine di stato di Cozmo a 64x64 pixel mantenendo
un batch-size pari a 256 sample, una proporzione 100 epoche/episodio. Queste modifiche hanno permesso non solo di
risolvere il problema legato alla "policy immobile", ma hanno ridotto drasticamente i tempi di attesa e conseguentemente
aumentato le possibilità di eseguire esperimenti e ottenere grafici in tempo ridotto rispetto al precedente.

### Save'n'Restore phase

L'implementazione precedente presentava una fase di salvataggio dello stato (e.g. reti, episodi, memoria) che poteva
essere avviata dall'utente che sceglieva deliberatamente di sospendere l'esperimento. Questo tipo di approccio, oltre a
presentare bug che sono stati prontamente risolti, rischiava di rendere vane ore di esperimenti: per quanto la
connessione tra Cozmo, Tablet e Pc sia stabile, ogni tanto capita che avvenga una disconnessione di natura
prettamente stocastica che provocava una repentina interruzione del programma con conseguente deterioramento dei dati
finora raccolti.

Onde evitare ulteriori problemi di questo tipo, si è ritenuto necessario implementare un meccanismo di checkpoint, dove
è il sistema a salvare lo stato dell'esperimento creandone una copia che può essere utile per ricominciare la 
procedura. L'operazione di salvataggio viene fatta ogni X episodi. In caso di interruzione indesiderata del programma,
gli episodi di learning persi saranno questa volta irrisori e si potrà ricominciare l'esperimento dall'ultimo
checkpoint. 

## Ulteriori sviluppi per la prossima settimana

### Alpha esplode

Alpha, il parametro che viene imparato in maniera automatizzata dall'algoritmo SAC, inizia da un valore pari a zero, ma
dopo un gran numero di iterazioni esplode verso l'alto in maniera quasi esponenziale. E' un'anomalia che non ho
riscontrato negli episodi del pendolo: devo verificare che sia davvero un'anomalia.

### Cozmo-DDPG

Implementazione dell'algoritmo di DDPG per Cozmo in modo da portare avanti gli esperimenti anche utilizzando questo
algoritmo già implementato con il pendolo.

### Esperiementi

Ora che l'implementazione permette di avere tempistiche di learning umane, l'intenzione è quella di andare a fare
esperimenti utilizzando differenti ambienti (e.g. single-lane, single-line) su almeno 300-500 episodi per poter vedere
come si evolve il learning e quindi verificare se il robot sta imparando o meno.