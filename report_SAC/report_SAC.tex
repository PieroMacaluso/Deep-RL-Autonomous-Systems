\documentclass[a4paper, 11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{vhistory}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{float}
\usepackage{blindtext}
\usepackage[inline]{enumitem}
\usepackage{xcolor}
\usepackage{amsmath, amssymb, lmodern}
\usepackage{array}
\usepackage{listings}
\usepackage{svg}
\usepackage{color}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{varioref}
\usepackage{hyperref} 
\usepackage[ruled,vlined]{algorithm2e}
\usepackage[section]{placeins}
\usepackage{comment} % enables the use of multi-line comments (\ifx \fi) 
\usepackage{graphicx}
\usepackage{lipsum} %This package just generates Lorem Ipsum filler text. 
\usepackage{fullpage} % changes the margin
\usepackage{cleveref}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}
%Includes "References" in the table of contents
\usepackage[nottoc]{tocbibind}

\DeclareMathOperator*{\argmax}{argmax} % thin space, limits underneath in displays

\newenvironment{conditions}
{\par\vspace{\abovedisplayskip}\noindent\begin{tabular}{>{$}l<{$} @{${}={}$} l}}
	{\end{tabular}\par\vspace{\belowdisplayskip}}

\renewcommand{\lstlistingname}{Algorithm}% Listing -> Algorithm
\renewcommand{\lstlistlistingname}{List of \lstlistingname s}% List of Listings -> List of Algorithms
\crefname{listing}{algorithm}{algorithms}  
\Crefname{listing}{Algorithm}{Algorithms}
\floatname{algorithm}{Procedure}

\lstset{frame=tb,
	language=Python,
	aboveskip=3mm,
	belowskip=3mm,
	showstringspaces=false,
	columns=flexible,
	basicstyle={\small\ttfamily},
	numbers=left,
	numberstyle=\tiny\color{gray},
	keywordstyle=\color{blue},
	commentstyle=\color{dkgreen},
	stringstyle=\color{mauve},
	breaklines=true,
	breakatwhitespace=true,
	tabsize=3,
	inputencoding=latin1
}

%\setlength{\belowcaptionskip}{-10pt}
\makeatletter
\renewcommand{\paragraph}{%
	\@startsection{paragraph}{4}%
	{\z@}{1.0ex \@plus 1ex \@minus .2ex}{-1em}%
	{\normalfont\normalsize\bfseries}%
}
\makeatother

\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{varwidth}
\usepgfplotslibrary{fillbetween}
\usetikzlibrary{calc,trees,positioning,arrows,chains,shapes.geometric,%
	decorations.pathreplacing,decorations.pathmorphing,shapes,%
	matrix,shapes.symbols}

\tikzstyle{object} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm,text centered, draw=white]
\tikzstyle{element} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm,text centered, draw=black]
\tikzstyle{xor} = [diamond, minimum width=1cm, minimum height=1cm, text centered, draw=black]
\tikzstyle{arrow} = [thick,->,>=stealth]

\pgfplotsset{width=\textwidth, height=\textheight*0.33,compat=newest}

\definecolor{train_color_1}{HTML}{8b402a}
\definecolor{train_color_2}{HTML}{e45e2d}
\definecolor{train_color_3}{HTML}{ef9c49}
\definecolor{train_color_4}{HTML}{fdea6f}

\definecolor{test_color_1}{HTML}{192574}
\definecolor{test_color_2}{HTML}{2e62a1}
\definecolor{test_color_3}{HTML}{43a7cb}
\definecolor{test_color_4}{HTML}{9ed5cd}

\pgfplotscreateplotcyclelist{train}{
	semithick,train_color_1\\%
	semithick,train_color_2\\%
	semithick,train_color_3\\%
	semithick,train_color_4\\%
}

\pgfplotscreateplotcyclelist{test}{
	semithick,test_color_1\\%
	semithick,test_color_2\\%
	semithick,test_color_3\\%
	semithick,test_color_4\\%
}



\captionsetup[figure]{skip=0pt}


\begin{document}
	%Header-Make sure you update this information!!!!
	\noindent
	\large\textbf{A Study of Reinforcement Learning} \hfill \textbf{Report SAC} \\
	\normalsize Prof. Pietro Michiardi - Eurecom \hfill   Piero Macaluso\\
	\normalsize Prof. Elena Baralis - Politecnico di Torino  \hfill Version: \vhCurrentVersion \space from \vhCurrentDate
	
	\tableofcontents
	\newpage
	\begin{versionhistory}
		\vhEntry{1.0}{April, 26 2019}{PM}{- Created.
			
			- Added SAC description and implementation.
			
			- Added Environments Description
			
			- Added Uniform/Prioritized Replay implementation and simulation.
		}
		%\vhEntry{1.1}{23.01.04}{DP|JPW}{correction}
		%\vhEntry{1.2}{03.02.04}{DP|JPW}{revised after review}
	\end{versionhistory}
	\newpage
	
	\section{Introduction} \label{introduction}
	
	After the analysis of \textbf{Deep Deterministic Policy Gradient (DDPG)} \cite{lillicrap2015continuous}, I decided to explore other algorithms. \textbf{Soft Actor-Critic (SAC)} \cite{haarnoja2018soft} \cite{haarnoja2018alg} is one of the algorithms that intrigued me because of the promises of its discoverers: higher and more stable performance than DDPG, stochastic framework and less parameter to tune.
	
	As will be clear later, the algorithm fully met the expectations.
	
	The aim of this report is to show a background of the algorithm, the performances obtained, a comparison with the performance of DDPG and possible future developments.
	
	\section{Soft Actor-Critic} \label{sac}
	\subsection{Application Field}
	% TODO: Check this

	SAC combines the off-policy actor-critic setup with a \textbf{stochastic policy (actor)}, devising a bridge between stochastic policy optimization and DDPG-style approaches.
	
	As DDPG, SAC can be applied to situations characterized by the presence of a continuous action spaces and it is a \textbf{Model-Free}, \textbf{Off-Policy} and \textbf{Actor-Critic} algorithm.
	
	SAC algorithm is able to overcome some of the problems of DDPG.
	The latter can achieve great performance, but the interaction between the deterministic actor network and the Q-function makes it difficult to stabilize and brittle with respect to hyper-parameters and other kinds of tuning. The learned Q-function begins to dramatically overestimate Q-values, which then leads to the policy breaking, because it exploits the errors in the Q-function.
	
	For this reason, SAC exploits \textbf{Clipped Double-Q Learning} used also by \textbf{Twin Delayed DDPG (TD3)}. It learns two Q-functions instead of one, and uses the smaller of the two Q-values to form the targets in the Bellman error loss functions.
	
	Another feature of SAC is \textbf{entropy regularization}. The policy is trained to maximize a trade-off between expected return and entropy, a measure of randomness in the policy. This is strongly related to the exploration-exploitation trade-off: increasing entropy results in more exploration, which can accelerate learning later on, but it can also prevent the policy from prematurely converging to a bad local optimum.
	
	\subsection{Key Points}
	\subsubsection{Reinforcement Learning Notation}
	The Reinforcement Learning Setup is the standard one. The problem can be defined as policy search in a Markov decision process (MDP), defined by a tuple $(\mathcal{S}, \mathcal{A}, p, r)$. The state space $\mathcal{S}$ and action space $\mathcal{A}$ are continuous and the state transition probability $p : \mathcal{S} \times \mathcal{S} \times \mathcal{A} \rightarrow [0,\infty)$ represents the probability density of the next state $s_{t+1} \in \mathcal{S}$ given the current state $s_t \in \mathcal{S}$ and action $a_t \in \mathcal{A}$. The environment emits a reward $r: \mathcal{S} \times \mathcal{A} \rightarrow [r_{\min}, r_{\max}]$ on each transition. $\rho_\pi(s_t)$ and $\rho_\pi(s_t, a_t)$ denote the state and state-action marginals of the trajectory ($\tau$) distribution induced by a policy $\pi(a_t|s_t)$.
	
	\subsubsection{Entropy-Regularized Reinforcement Learning}
	
	\textbf{Entropy} is the average rate at which information is produced by a stochastic source of data. It is, in simple terms, a quantity which describes how random a random variable is. 
	The motivation behind the use of entropy is that when the data source produces a low-probability value (rare), the event carries \textbf{more \textit{information}} than when the source data produces a high-probability value.
	
	Let $x$ be a random variable with probability mass or density function $P$. The entropy $\mathcal{H}$ of $x$ is computed from its distribution $P$ according to
	
	\begin{equation} \label{eq1:entropy}
	\mathcal{H}(P) = \mathbb{E}_{x \sim P} [- \log P(x)]
	\end{equation}.
	
	In \textbf{entropy-regularized reinforcement learning} the standard objective is generalized by augmenting it with entropy. The agent gets a bonus reward at each time step proportional to the entropy of the policy at that timestep. Assuming an infinite-horizon discounted setting, this changes the RL problem to:
	\begin{equation} \label{eq2:entropy_return}
		\pi^* = \arg \max_{\pi} \mathbb{E}_{\tau \sim \pi}\Bigg[\sum_{t=0}^{\infty} \gamma^t \bigg(R(s_t, a_t, s_{t+1}) + \alpha \mathcal{H}(\pi(\cdot|s_t))\bigg)\Bigg]
	\end{equation}	
	
	where $\alpha > 0$ is the temperature parameter that determines the relative importance of the entropy term controlling the stochasticity of the optimal policy. It is clear that the standard maximum expected return can be retrieved in the limit as $\alpha \rightarrow 0$.
	
	From \vref{eq2:entropy_return} we can derive \textbf{state-value function} $V^\pi(s)$ and \textbf{action-value function} $Q^\pi(s,a)$:
	
	\begin{equation} \label{eq3:state_value}
	V^\pi(s) = \mathbb{E}_{\tau \sim \pi}\Bigg[\sum_{t=0}^{\infty} \gamma^t \bigg(R(s_t, a_t, s_{t+1}) + \alpha \mathcal{H}(\pi(\cdot|s_t))\bigg)\bigg|s_0 = s\Bigg]
	\end{equation}	
	
	\begin{equation} \label{eq4:action_value}
	Q^\pi(s,a) = \mathbb{E}_{\tau \sim \pi}\Bigg[\sum_{t=0}^{\infty} \gamma^t R(s_t, a_t, s_{t+1}) + \alpha \sum_{t=1}^{\infty} \gamma^t \mathcal{H}(\pi(\cdot|s_t))\bigg|s_0 = s, a_0 =a\Bigg]
	\end{equation}	
	
	From these equations is possible to derive the connection between state-value and action-value function given by
	\begin{equation} \label{eq5:q_v_relation}
	V^\pi(s) = \mathbb{E}_{a\sim\pi}[Q^\pi(s,a)] + \alpha \mathcal{H}(\pi(\cdot|s))
	\end{equation}
	
	and the \textbf{Bellman equation} given by

	\begin{align} 	\label{eq6:bellman}
	Q^\pi(s,a) &= \mathbb{E}_{s'\sim P, a'\sim\pi}[R(s,a,s') + \gamma(Q^\pi(s',a') + \alpha \mathcal{H}(\pi(\cdot|s')))]\\
	&= \mathbb{E}_{s'\sim P}[R(s,a,s') + \gamma V^\pi(s')]
	\end{align}
	
	\subsubsection{Learning Equations}
	SAC algorithm learns a \textbf{policy} $\pi_\theta$, two Q-functions $Q_{\phi_1}$,  $Q_{\phi_2}$ and a value function $V_\psi$.
	
	\paragraph{Learning Q}The Q-functions are learned by Mean Squared Bellman Error (MSBE) minimization, using a target value network to form the Bellman backups. They both use the same target and have loss functions:
	
	\begin{equation}
	L(\phi_i, {\mathcal D}) = \mathbb{E}_{(s,a,r,s',d) \sim \mathcal D}\left[ \Bigg( Q_{\phi_i}(s,a) - \left(r + \gamma (1 - d) V_{\psi_{\text{targ}}}(s') \right) \Bigg)^2 \right].
	\end{equation}
	
	
	The target value network, like the target networks in DDPG, is obtained by polyak averaging the value network parameters over the course of training.
	
	\paragraph{Learning V} The value function is learned by exploiting a sample-based approximation of the connection  given by \vref{eq5:q_v_relation}. Let’s first rewrite the connection equation by using the definition of entropy to obtain:
	
	
	\begin{align}
	V^{\pi}(s) &= \mathbb{E}_{a \sim \pi}[Q^{\pi}(s,a)] + \alpha \mathcal{H} \left(\pi(\cdot|s)\right) \\ &= \mathbb{E}_{a \sim \pi}[Q^{\pi}(s,a) - \alpha \log \pi(a|s)].
	\end{align}
	
	The RHS
	
	
	For this reason, SAC exploits \textbf{Clipped Double-Q Learning} used also by \textbf{Twin Delayed DDPG (TD3)}. It learns two Q-functions instead of one, and uses the smaller of the two Q-values to form the targets in the Bellman error loss functions.
	
	
	The implementation of these updates can be found in \vref{lst:update}.
	\lstinputlisting[linerange={281-306},firstnumber=281,label={lst:update},caption={Updating Critic, Actor and Target Networks}]{code/AgentDDPG.py}

	
	
	\subsubsection{Replay Buffers} Most optimization algorithms assume that the samples are \textbf{independently and identically distributed (i.i.d)}, but data produced sequentially exploring the environment can not satisfy this assumption. To solve this problem, a Replay Buffer can be used: it is a set $\mathcal D$ of $N$ recent experiences $(s_t, a_t, r_t, s_{t+1}, d_t)$ from which the algorithm will randomly sample a subset of $M\ll N$ experiences (mini-batch) at each iteration.
	
	The replay buffer should be large enough to contain a wide range of experiences in order to have stable algorithm behavior, but it may not always be good to keep everything. Using only the very-most recent data leads to overfitting, while using too much experience may slow down the learning process.
	
	The first approach is to select the mini-batch sampling uniformly among all the entries in the Replay Buffer.
	A more complex approach is the one using a \textbf{Prioritized Buffer Replay} \cite{schaul2015prioritized} where the samples with high expected learning progress are replayed more frequently. This prioritization can lead to a loss of diversity, which can be alleviated by \textbf{stochastic prioritization}, and a bias that can be corrected by \textbf{importance sampling}.
	
	In \vref{results} the results of the two approaches will be analyzed.
	
	\subsubsection{Target Networks} 
	\begin{figure}[!h]
		\begin{center}
			\begin{tikzpicture}
			[node distance=1.5cm]
			\node[object] (state)  {State $s_t$ };
			\node[element, below of=state] (localActor){ Actor Network $\mu(s|\theta^\mu)$};
			\node[object, below of=localActor] (action){ Action $a_t$};
			
			\draw[arrow] (state) -- (localActor);
			\draw[arrow] (localActor) -- (action);
			\end{tikzpicture}
			\begin{tikzpicture}
			[node distance=1.5cm]
			\node[object, left of=action] (state)  {State $s_t$ };
			\node[object, right of=state] (action)  {Action $a_t$};
			\node[element,below = of $(state)!0.5!(action)$] (localCritic){ Critic Local $Q(s,a|\theta^Q)$};
			\node[object, below of=localCritic] (value){ Q-value $q_t$};
			
			\draw[arrow] (state) -- (localCritic);
			\draw[arrow] (action) -- (localCritic);
			\draw[arrow] (localCritic) -- (value);
			%\draw [draw, -latex',thick] (3.east) -- ++(2,0) node(lowerright){} |- (state.east);
			
			\end{tikzpicture}
		\end{center}		
		\caption{Actor and Critic Networks}
		\label{fig:actor_critic}
	\end{figure}
	In DDPG we have 4 neural networks: the \textbf{local Actor}, the \textbf{local Critic}, the \textbf{target Actor} and the \textbf{target Critic}. The aim of Actor networks is to approximate the \textbf{Policy} while the Critic networks approximate the \textbf{Q-Value}.
	
	Initially Actors and Critics have the same randomly initialized weights. Then the local Actor (the current policy) starts to propose actions to the Agent, given the current state, starting to populate the Replay Buffer of experiences.
	
	When the Replay Buffer is big enough, the algorithm starts to sample randomly a mini-batch of experiences for each timestep $t$. This mini-batch is used to update the local Critic minimizing the Mean Squared Loss between the local Q-value and the target one (\vref{eq9:loss}) and to update the actor policy using the sampled policy gradient (\vref{eq8:mean_gradient}).
	
	\begin{equation}\label{eq9:loss}
	L = \frac{1}{N} \sum_i(y_i -Q(s_i, a_i|\theta^Q))^2
	\end{equation}\label{eq10:gradient}
	where $y_i$ is given by \vref{eq5:yt}.
	
	We can imagine the target networks as the \textit{labels} of supervised learning.
	
	Also the target networks are updated in this \textit{learning step}. A mere copy of the local weights is not an efficient solution, because it is prone to divergence. For this reason, a "soft" target updates is used. It is given by \[\theta' \leftarrow \tau\theta' \leftarrow \tau \theta + (1-\tau)\theta'\] with $t \ll 1$.
	
	The pseudo-code of this procedure is shown in \vref{ddpgalg}
	
	\subsubsection{Exploration vs. Exploitation} 
	In Reinforcement learning for discrete action spaces, exploration is done selecting a random action (e.g.\ epsilon-greedy). For continuous action spaces, exploration is done adding noise to the action itself. In \cite{lillicrap2015continuous}, the authors use Ornstein-Uhlenbeck Process \cite{uhlenbeck1930theory} to add noise to the action output $
	a_t = \mu(s_t|\theta^\mu) + \mathcal{N}$. After that the action is clipped in the correct range.
	 
	\subsection{Steps made}
	The initial step was trying to implement the algorithm in \cite{lillicrap2015continuous} using two simple environment provided by OpenAI Gym: \textit{MountainCarContinuous-v0} and \textit{Pendulum-v0}. In this report we will use only shallow Neural Networks implemented as shown in \vref{fig:actor_critic_schema}, because the state is represented directly by the data of the observation.
	
	The next goal will be to apply a Convolutional Neural Network (CNN) to states represented by a set of RGB images of the same environments.
	
	In this report the algorithm was implemented using a simple Replay Buffer and a Prioritized one in order to understand better the efficiency and the differences. In \vref{results} is possible to observe the results and the comparison of these two approach.
	
	\subsubsection{Hyper Parameters}
	The aim of this section is to describe the Hyper Parameters of DDPG.
	
	\begin{description}
		\item[Epsilon (\texttt{eps\_start}, \texttt{eps\_end}, \texttt{eps\_decay})] it is described by the function \[\epsilon = \epsilon_{\text{start}} - (\epsilon_{\text{start}} -\epsilon_{\text{end}})\min(1.0, \frac{e}{\epsilon_{\text{decay}}})\] where $e$ is the current episode number. It is used to decrease the impact of the noise on the actions in function of the number of episode. When it reaches the \texttt{eps\_end}, it will become a constant.
		
		% \texttt{default: [eps\_start = 0.9, eps\_end = 0.2, eps\_decay = max\_episode]}
		
		\item[Noise (\texttt{mu}, \texttt{sigma}, \texttt{theta})] these are the Ornstein-Uhlenbeck Process Noise parameters.
		
		% \texttt{default: [mu = 0.0, sigma = 0.3, theta = 0.15]}
		
		\item[Replay (\texttt{batch\_size}, \texttt{replay\_min\_size}, \texttt{replay\_max\_size})] \texttt{batch\_size} is the dimension of the mini-batch sampled by the memory. The learning process starts when the replay memory contains at least \texttt{replay\_min\_size} transitions and it starts to overwrite old transitions when it reaches \texttt{replay\_max\_size}.
		
		% \texttt{default: [batch\_size = 100, replay\_min\_size = 10000, replay\_max\_size = 1000000]}
		
		\item[Episode (\texttt{n\_episode}, \texttt{episode\_max\_len})] the number of episode for each run is \texttt{n\_episode}, while the maximum length of an episode is \texttt{episode\_max\_len}.
		
		% \texttt{default: [n\_episode = 300-500, episode\_max\_len = 300-1000]}
		
		\item[Neural Networks (\texttt{weight\_decay}, \texttt{update\_method}, \texttt{lr})] set of parameter for each network. The first parameter is always set to 0 and never used. The second one is always set to Adaptive Moment Estimation (ADAM), while the third is the learning rate and it is usually set to \texttt{1e-3} or \texttt{1e-4}.
		
		% \texttt{default: [weight\_decay = 0, update\_method = 'adam', lr = 1e-3,1e-4]}
		
		\item[Update (\texttt{discount}, \texttt{soft\_target\_tau}, \texttt{n\_updates\_per\_step})] \texttt{discount} is $\gamma$, \texttt{soft\_target\_tau} is $\tau$, while \texttt{n\_updates\_per\_step} is the number of times that the algorithm has to extract a mini-batch and perform the update of the networks for each timestep.
		
		% \texttt{default: [discount = 0.99, soft\_target\_tau = 0.001, n\_updates\_per\_episode = 1]}
		
		\item[Test (\texttt{n\_tests}, \texttt{every\_n\_episode})] \texttt{n\_tests} is the number of episode to test in the testing phase, while \texttt{every\_n\_episode} indicates how often the testing phase starts. 
		
		% \texttt{default: [n\_tests = 10]}
		
		
	\end{description}
\begin{algorithm}[!h]
	\SetAlgoLined
	\DontPrintSemicolon
	\LinesNumbered
	\KwIn{Initial policy parameters $\theta$, Q-function parameters $\phi_1$, $\phi_2$, V-function parameters $\psi$, empty replay buffer $\mathcal{D}$}
	Set target parameters equal to main parameters $\psi_{targ} \leftarrow \psi$\;
	\Repeat{\text{convergence}}{
		Observe state $s$ and select action $a \sim \pi_\theta(\cdot|s)$\;
		Execute $a$ in the environment\;
		Observe next state $s'$, reward $r$ and done signal $d$\;
		Store $(s,a,r,s',d)$ in replay buffer $\mathcal{D}$\;
		If $s'$ is terminal, reset the environment state.\;
		\If{size of $\mathcal{D} >$ warm\_up\_threshold}{
			\For{j in range(\#updates\_per\_step)}{
				Ramdomly sample a batch of transitions, $B = {(s,a,r,s',d)}$ from $\mathcal{D}$\;
				Compute targets for Q and V functions:
				\begin{align}
				y_q(r,s',d) &= r +\gamma(1-d)V_{\psi_{targ}}(s')\\
				y_v(s) &= \min_{i=1,2}{Q_{\phi_i}(s, \tilde{a})-\alpha \log{\pi_\theta(\tilde{a}|s)}, \quad \tilde{a} \sim \pi_\theta (\cdot|s)}
				\end{align}\;
				Update Q-functions by one step of gradient descent using
				\[
				\nabla_{\phi_i}\frac{1}{|B|}\sum_{(s,a,r,s',d) \in B} (Q_{\phi,i}(s,a) - y_q(r,s',d))^2 \qquad \text{for } i = 1, 2
				\]\;
				Update V-function by one step of gradient descent using
				\[
				\nabla_{\psi}\frac{1}{|B|}\sum_{s \in B} (V_\psi(s)-y_v(s))^2
				\]\;
				Update policy by one step of gradient ascent using
				\[
				\nabla_{\theta}\frac{1}{|B|}\sum_{s \in B} \Bigg(Q_{\phi,1}(s, \tilde{a_\theta(s)}) - \alpha \log{\pi_\theta(\tilde{a_\theta(s)}|s)}\Bigg)
				\],
				where $a_\theta(s)$ is a sample from $\pi_\theta(\cdot|s)$ which is differentiable wrt $\theta$ via the reparametrization trick.\;
				Update target value network with:
				\[
				\psi_{targ} \leftarrow \tau \psi_{targ} + (1-\tau)\psi
				\]
				}
			}
	}
	
	\caption{Soft Actor-Critic}
	\label{sacalg}
\end{algorithm}
\begin{figure}[!h]
	\begin{center}
		\begin{tikzpicture}
		[node distance=1.5cm]
		\node[object] (state)  {State $s_t$};
		\node[element, below of=state] (linear1){\texttt{ReLu(Linear(in=$S$, out=256))}
		};
		\node[element, below of=linear1] (linear2){\texttt{ReLu(Linear(in=256, out=256))}
		};
		\node[element, below of=linear2] (linear3){\texttt{Tanh(Linear(in=256, out=$A$))}
		};
		\node[object, below of=linear3] (action){ Action $a_t$};
		
		\draw[arrow] (state) -- (linear1);
		\draw[arrow] (linear1) -- (linear2);
		\draw[arrow] (linear2) -- (linear3);
		\draw[arrow] (linear3) -- (action);
		\end{tikzpicture}
		\begin{tikzpicture}
		[node distance=1.5cm]
		\node[object, left of=action] (state)  {State $s_t$ };
		\node[object, right of=state] (action)  {Action $a_t$};
		\node[element, below= of $(state)!0.5!(action)$] (linear1){\texttt{ReLu(Linear(in=$S+A$, out=256))}
		};
		\node[element, below of=linear1] (linear2){\texttt{ReLu(Linear(in=256, out=256))}
		};
		\node[element, below of=linear2] (linear3){\texttt{Linear(in=256, out=1)}
		};
		\node[object, below of=linear3] (value){ Q-value $q_t$};
		
		\draw[arrow] (state) -- (linear1);
		\draw[arrow] (action) -- (linear1);
		\draw[arrow] (linear1) -- (linear2);
		\draw[arrow] (linear2) -- (linear3);
		\draw[arrow] (linear3) -- (value);
		%\draw [draw, -latex',thick] (3.east) -- ++(2,0) node(lowerright){} |- (state.east);
		
		\end{tikzpicture}
	\end{center}		
	\caption{Actor and Critic Networks: $S$ is the length of the array of states, while $A$ is the length of the array of actions.}
	\label{fig:actor_critic_schema}
\end{figure}
	\section{OpenAI Gym Environments}
	\subsection{MountainCarContinuous-v0} 
	\subsubsection{Description}
	An underpowered car must climb a one-dimensional hill to reach a target. The action (engine force applied) is a continuous value. 
	The target is on top of a hill on the right-hand side of the car. If the car reaches it or goes beyond, the episode terminates. On the left-hand side, there is another hill. Climbing this hill can be used to gain potential energy and accelerate towards the target. On top of this second hill, the car cannot go further than a position equal to -1, as if there was a wall. Hitting this limit does not generate a penalty.
	%\begin{figure}[ht!]
	%	\centering
	%	\includegraphics[height=0.2\paperwidth]{img/mountaincar.png}
	%	\caption{Frame of MountainCarContinuous-v0 environment}
	%	\label{fig:mountaincar}
	%\end{figure}
	\paragraph{Observation}
	Type: Box(2)
	\begin{table}[!h]
		\centering
		% \caption{MountainCarContinuous-v0 Observation}
		\label{mountain_observation}
		\begin{tabular}{@{}lllll@{}}
			\toprule
			Index	& Observation		& Min 		& Max 		\\ \midrule
			0			& Car Position	 	&  $-1.2$	& $+0.6$ 	\\
			1			& Car Velocity	 	&  $-0.07$		& $+0.07$ \\
			\bottomrule
		\end{tabular}
	\end{table}
	\paragraph{Actions}
	Type: Box(1)
	\begin{table}[!h]
		\centering
		% \caption{MountainCarContinuous-v0 Actions }
		\label{mountain_action}
		\begin{tabular}{@{}lllll@{}}
			\toprule
			Index	& Action	& Min 		& Max 		\\ \midrule
			0			& Push car to the left (negative value) or to the right (positive value)&  $-1$	& $+1$ 	\\
			\bottomrule
		\end{tabular}
	\end{table}
	\paragraph{Reward}
	The reward for each timestep $t$ is given by $r_t = d_t*100 - a_t^2*0.1$
	where $d_t$ is the done flag and $a_t$ is the continuous value of the action taken.
	
	This reward function raises an exploration challenge, because if the agent does not reach the target soon enough, it will figure out that it is better not to move, and won't find the target anymore.
	
	\paragraph{Starting State}
	Position between -0.6 and -0.4, null velocity.
	\paragraph{Episode Termination}
	Position equal to 0.5.
	\paragraph{Solved Requirements}
	Get a reward over 90.
	
		\subsubsection{Hyper-Parameters Used}
	\begin{table}[!h]
		\centering
		% \caption{MountainCarContinuous-v0 Actions }
		\label{mountain_action}
		\begin{tabular}{@{}lllllll@{}}
			\toprule
		Type			&Parameter			& Value	& Parameter				& Value  & Parameter			&  Value 	\\		\midrule
		Epsilon & \texttt{eps\_start}	& $0.9$	&  \texttt{eps\_end}	& $0.2$ & \texttt{eps\_decay} 	& $300$		\\
		Noise & \texttt{mu}	& $0.0$	&  \texttt{sigma}	& $0.3$ & \texttt{theta} 	& $0.15$		\\
		Replay & \texttt{batch\_size}	& $100|32$	&  \texttt{replay\_min\_size}	& $10^4$ & \texttt{replay\_max\_size} 	& $10^6$		\\
		Episode & \texttt{n\_episode}	& $300$	&  \texttt{episode\_max\_len}	& $1000$ & 	& 		\\
		Networks & \texttt{weight\_decay}	& $0.0$	&  \texttt{update\_method}	& 'adam' & \texttt{lr} 	& $1e^{-4}$		\\
		Update & \texttt{discount}	& $0.99$	&  \texttt{soft\_target\_tau}	& $0.001$ & \texttt{n\_updates\_per\_step} 	& $1$		\\
		Test & \texttt{n\_tests}	& $100$	&  \texttt{every\_n\_episode}	& $10$ &  	& 	\\
		\bottomrule
		\end{tabular}
	\end{table}
	
	\subsection{Pendulum-v0} 
	\subsubsection{Description}
	The inverted pendulum swingup problem is a classic problem in the control literature. In this version of the problem, the pendulum starts in a random position, and the goal is to swing it up so it stays upright.
	%\begin{figure}[ht!]
	%	\centering
	%	\includegraphics[height=0.2\paperwidth]{img/pendulum.png}
	%	\caption{Frame of Pendulum-v0 environment}
	%	\label{fig:pendulum}
	%\end{figure}
	\paragraph{Observation}
	Type: Box(3)
	\begin{table}[!h]
		\centering
		% \caption{MountainCarContinuous-v0 Observation}
		\label{mountain_observation}
		\begin{tabular}{@{}lllll@{}}
			\toprule
			Index	& Observation		& Min 		& Max 		\\ \midrule
			0			& cos($\theta$)	 	&  $-1.0$	& $+1.0$ 	\\
			1			& sin($\theta$)	 	&  $-1.0$		& $+1.0$ \\
			2			& $\dot{\theta}$	 	&  $-8.0$		& $+8.0$ \\
			\bottomrule
		\end{tabular}
	\end{table}
	\paragraph{Actions}
	Type: Box(1)
	\begin{table}[!h]
		\centering
		% \caption{MountainCarContinuous-v0 Actions }
		\label{mountain_action}
		\begin{tabular}{@{}lllll@{}}
			\toprule
			Index	& Action	& Min 		& Max 		\\ \midrule
			0			& Joint effort &  $-2.0$	& $+2.0$ 	\\
			\bottomrule
		\end{tabular}
	\end{table}
	
	\paragraph{Reward}
	The reward for each timestep $t$ is given by \[r_t = -(\theta_t^2 + 0.1 \dot{\theta}^2 + 0.001 a_t^2)\]
	where theta is normalized between $-\pi$ and $\pi$. Therefore, the lowest cost is $-(\pi^2 + 0.1*8^2 + 0.001*2^2) = -16.2736044$, and the highest cost is $0$. In essence, the goal is to remain at zero angle (vertical), with the least rotational velocity, and the least effort.
	
	\paragraph{Starting State}
	Random angle from $-\pi$ to $\pi$, and random velocity between $-1$ and $1$
	\paragraph{Episode Termination}
	There is no specified termination. Adding a maximum number of steps might be a good idea. In this case 200.
	\paragraph{Solved Requirements}
	It is an unsolved environment, which means it does not have a specified reward threshold at which it is considered solved.
	
	\subsubsection{Hyper-Parameters Used}
	\begin{table}[!h]
		\centering
		% \caption{MountainCarContinuous-v0 Actions }
		\label{mountain_action}
		\begin{tabular}{@{}lllllll@{}}
			\toprule
			Type			&Parameter			& Value	& Parameter				& Value  & Parameter			&  Value 	\\		\midrule
			Epsilon & \texttt{eps\_start}	& $0.9$	&  \texttt{eps\_end}	& $0.2$ & \texttt{eps\_decay} 	& $300$		\\
			Noise & \texttt{mu}	& $0.0$	&  \texttt{sigma}	& $0.3$ & \texttt{theta} 	& $0.15$		\\
			Replay & \texttt{batch\_size}	& $30$	&  \texttt{replay\_min\_size}	& $2500$ & \texttt{replay\_max\_size} 	& $10^6$		\\
			Episode & \texttt{n\_episode}	& $300$	&  \texttt{episode\_max\_len}	& $200$ & 	& 		\\
			Networks & \texttt{weight\_decay}	& $0.0$	&  \texttt{update\_method}	& 'adam' & \texttt{lr} 	& $1e^{-4}$		\\
			Update & \texttt{discount}	& $0.99$	&  \texttt{soft\_target\_tau}	& $0.001$ & \texttt{n\_updates\_per\_step} 	& $1$		\\
			Test & \texttt{n\_tests}	& $100$	&  \texttt{every\_n\_episode}	& $10$ &  	& 	\\
			\bottomrule
		\end{tabular}
	\end{table}
	\newpage
	\section{Comparing Results} \label{results}
	
	In order to better evaluate the performances of these algorithms, \textbf{TensorboardX} was used. 
	The mean $\mu$, $min$,  $max$ and standard deviation $\sigma$ were calculated with a tool and the important areas they describe were plotted for better visualization.
	\paragraph{\color{train_color_2}{Training Phase}} The training phase was repeated 20 times for \texttt{n\_episode} episodes and the results were used to calculate aggregate values.
	\paragraph{\color{test_color_2}{Test Phase}} After \texttt{every\_n\_episode} episodes, the test phase was triggered. In this part the current actor network was set in evaluation mode and tested on 100 random episodes. Also these results were used to calculate aggregate values.
	
	In the first 10 episodes, the selection of the actions to take are sampled from a uniform random distribution over valid actions. This is a way to improve exploration in the first steps. After that, it returns to normal DDPG/SAC exploration.
	
	\subsection{Durations}
	\begin{table}[!h]
		\centering
		% \caption{MountainCarContinuous-v0 Actions }
		\label{mountain_action}
		\begin{tabular}{@{}lllllll@{}}
			\toprule
			Environment			&\multicolumn{2}{ c }{Uniform Replay Memory}	& \multicolumn{2}{ c }{Prioritized Replay Memory}\\		
			& One & Total & One & Total\\ \midrule
			MountainCarContinuous-v0	& 15 min	&  5 h	& 13 min & 4.5 h		\\
			Pendulum-v0	&  6 min	&  2 h	& 6 min & 2 h		\\
			\bottomrule
		\end{tabular}
	\end{table}
	
	\subsection{MountainCarContinuous-v0} \label{mountain}
	\subsubsection{Uniform Replay Memory} 
	\begin{figure}[h]
		\begin{tikzpicture}
		\begin{axis}[axis on top, 
		xmin=1,
		xmax=300,
		ymin=-50,
		ymax=100,
		height=\textheight*0.40,
		set layers=standard,
		cycle list name=train,
		grid=both,
		grid style={solid,gray!30!white},
		% axis lines=middle,
		xlabel=Episode,
		ylabel style={align=center}, ylabel=Reward Value,
		%legend style={at={(0.98,0.2)},anchor=east},
		legend pos=south east,
		 extra y ticks = {90},
		 	extra y tick style={grid=major, grid style={solid,green},y tick label style={
		 		/pgf/number format/.cd,precision=10
		}},
		% x label style={at={(axis description cs:0.5,0)},anchor=north},
		%y label style={at={(axis description cs:-0.1,.5)},rotate=90,anchor=south}
		]
		
		\addplot table[x=Step,y=Value, col sep=comma] {plot/replayBuffer/mountain-v0/train/episode/mean.csv};
		\addlegendentry{Mean $\mu$ Reward Episode};
		
		\addplot [name path=upper,draw=none, forget plot] table[x=Step,y expr=\thisrow{Max}, col sep=comma] {plot/replayBuffer/mountain-v0/train/episode/mean.csv};
		
		\addplot [name path=lower,draw=none, forget plot] table[x=Step,y expr=\thisrow{Min}, col sep=comma] {plot/replayBuffer/mountain-v0/train/episode/mean.csv};
		
		\addplot [fill=train_color_3] fill between[of=upper and lower];
		\addlegendentry{Area $[min, max]$};
		
		\addplot [name path=upper1,draw=none, area legend, forget plot] table[x=Step,y expr=\thisrow{Value}+\thisrow{Error}, col sep=comma] {plot/replayBuffer/mountain-v0/train/episode/mean.csv};
		
		\addplot [name path=lower1,draw=none, area legend, forget plot] table[x=Step,y expr=\thisrow{Value}-\thisrow{Error}, col sep=comma] {plot/replayBuffer/mountain-v0/train/episode/mean.csv};
		
		\addplot [fill=train_color_2] fill between[of=upper1 and lower1];
		\addlegendentry{Area $[\mu-\sigma, \mu+\sigma]$};

		
		
		\end{axis}
		\end{tikzpicture}
		\caption{Mean, Standard Deviation Range and Min-Max range of the reward of each episode over 20 runs. }
		\label{fig:mountain_r_train_episode}
	\end{figure}
	\begin{figure}[h]
		\begin{tikzpicture}
		\begin{axis}[axis on top, 
		xmin=1,
		xmax=300,
		ymin=-50,
		ymax=100,
		height=\textheight*0.40,
		set layers=standard,
		cycle list name=train,
		grid=both,
		grid style={solid,gray!30!white},
		% axis lines=middle,
		xlabel=Episode,
		ylabel style={align=center}, ylabel=Reward Value,
		%legend style={at={(0.98,0.2)},anchor=east},
		legend pos=south east,
		extra y ticks = {90},
		 	extra y tick style={grid=major, grid style={solid,green},y tick label style={
		 		/pgf/number format/.cd,precision=10
		}},
		% x label style={at={(axis description cs:0.5,0)},anchor=north},
		%y label style={at={(axis description cs:-0.1,.5)},rotate=90,anchor=south}
		]
		
		\addplot table[x=Step,y=Value, col sep=comma] {plot/replayBuffer/mountain-v0/train/meanLast100/mean.csv};
		%\addlegendentry{Mean Reward of last 100 episode};
		
		\addplot [name path=upper,draw=none, forget plot] table[x=Step,y expr=\thisrow{Max}, col sep=comma] {plot/replayBuffer/mountain-v0/train/meanLast100/mean.csv};
		
		\addplot [name path=lower,draw=none, forget plot] table[x=Step,y expr=\thisrow{Min}, col sep=comma] {plot/replayBuffer/mountain-v0/train/meanLast100/mean.csv};
		
		\addplot [fill=train_color_3] fill between[of=upper and lower];
		
		\addplot [name path=upper1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Value}+\thisrow{Error}, col sep=comma] {plot/replayBuffer/mountain-v0/train/meanLast100/mean.csv};
		
		\addplot [name path=lower1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Value}-\thisrow{Error}, col sep=comma] {plot/replayBuffer/mountain-v0/train/meanLast100/mean.csv};
		
		\addplot [fill=train_color_2] fill between[of=upper1 and lower1];
		
		\addlegendentry{Mean $\mu$ Reward Last 100};
		\addlegendentry{Area $[min, max]$};
		\addlegendentry{Area $[\mu-\sigma, \mu+\sigma]$};
		\end{axis}
		\end{tikzpicture}
		\caption{Mean, Standard Deviation Range and Min-Max range of the running reward mean of the last 100 episodes for each episode over 20 runs.}
		\label{fig:mountain_r_train_mean}
	\end{figure}
	\begin{figure}[h]
		\begin{tikzpicture}
		\begin{axis}[axis on top, 
		xmin=10,
		xmax=300,
		ymin=-50,
		ymax=100,
		height=\textheight*0.40,
		set layers=standard,
		cycle list name=test,
		grid=both,
		grid style={solid,gray!30!white},
		% axis lines=middle,
		xlabel=Episode,
		ylabel style={align=center}, ylabel=Reward Value,
		%legend style={at={(0.98,0.2)},anchor=east},
		legend pos=south east,
		 extra y ticks = {90},
		 	extra y tick style={grid=major, grid style={solid,green},y tick label style={
		 		/pgf/number format/.cd,precision=10
		}},
		% x label style={at={(axis description cs:0.5,0)},anchor=north},
		%y label style={at={(axis description cs:-0.1,.5)},rotate=90,anchor=south}
		]
		
		\addplot table[x=Step,y=Value, col sep=comma, fill=test_color_1] {plot/replayBuffer/mountain-v0/test/rewardMean/mean.csv};
		%\addlegendentry{Mean Reward of last 100 episode};
		
		\addplot [name path=upper,draw=none, forget plot] table[x=Step,y expr=\thisrow{Max}, col sep=comma] {plot/replayBuffer/mountain-v0/test/rewardMean/mean.csv};
		
		\addplot [name path=lower,draw=none, forget plot] table[x=Step,y expr=\thisrow{Min}, col sep=comma] {plot/replayBuffer/mountain-v0/test/rewardMean/mean.csv};
		
		\addplot [fill=test_color_3] fill between[of=upper and lower];
		
		\addplot [name path=upper1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Value}+\thisrow{Error}, col sep=comma] {plot/replayBuffer/mountain-v0/test/rewardMean/mean.csv};
		
		\addplot [name path=lower1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Value}-\thisrow{Error}, col sep=comma] {plot/replayBuffer/mountain-v0/test/rewardMean/mean.csv};
		
		\addplot [fill=test_color_2] fill between[of=upper1 and lower1];
		
		\addlegendentry{Mean $\mu$ Test Reward};
		\addlegendentry{Area $[min, max]$};
		\addlegendentry{Area $[\mu-\sigma, \mu+\sigma]$};
		
		
		\end{axis}
		\end{tikzpicture}
		\caption{Mean, Standard Deviation Range and Min-Max range of reward mean of the test phase (every 10 episodes) over 20 runs.}
		\label{fig:mountain_r_test}
	\end{figure}
	\begin{figure}[h]
		\begin{tikzpicture}
		\begin{axis}[axis on top, 
		xmin=10,
		xmax=300,
		ymin=0,
		ymax=1000,
		height=\textheight*0.40,
		set layers=standard,
		cycle list name=test,
		grid=both,
		grid style={solid,gray!30!white},
		% axis lines=middle,
		xlabel=Episode,
		ylabel style={align=center}, ylabel=Reward Value,
		%legend style={at={(0.98,0.2)},anchor=east},
		legend pos=north east,
		%extra y ticks = {90},
		%extra y tick style={grid=major, grid style={solid,green},y tick label style={
		%		/pgf/number format/.cd,precision=10
		%}},
		% x label style={at={(axis description cs:0.5,0)},anchor=north},
		%y label style={at={(axis description cs:-0.1,.5)},rotate=90,anchor=south}
		]
		
		\addplot table[x=Step,y=Value, col sep=comma, fill=test_color_1] {plot/replayBuffer/mountain-v0/test/steps/mean.csv};
		%\addlegendentry{Mean Reward of last 100 episode};
		
		\addplot [name path=upper,draw=none, forget plot] table[x=Step,y expr=\thisrow{Max}, col sep=comma] {plot/replayBuffer/mountain-v0/test/steps/mean.csv};
		
		\addplot [name path=lower,draw=none, forget plot] table[x=Step,y expr=\thisrow{Min}, col sep=comma] {plot/replayBuffer/mountain-v0/test/steps/mean.csv};
		
		\addplot [fill=test_color_3] fill between[of=upper and lower];
		
		\addplot [name path=upper1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Value}+\thisrow{Error}, col sep=comma] {plot/replayBuffer/mountain-v0/test/steps/mean.csv};
		
		\addplot [name path=lower1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Value}-\thisrow{Error}, col sep=comma] {plot/replayBuffer/mountain-v0/test/steps/mean.csv};
		
		\addplot [fill=test_color_2] fill between[of=upper1 and lower1];
		
		\addlegendentry{Mean $\mu$ Test Step};
		\addlegendentry{Area $[min, max]$};
		\addlegendentry{Area $[\mu-\sigma, \mu+\sigma]$};
		
		
		\end{axis}
		\end{tikzpicture}
		\caption{Mean, Standard Deviation Range and Min-Max range of the number of steps in the test phase (every 10 episodes) over 20 runs.}
		\label{fig:mountain_r_test_step}
	\end{figure}
	\FloatBarrier
	\subsubsection{Prioritized Replay Memory} 
	In this case the size of the mini-batch was reduced from 100 to 32 in order to make the learning faster.
	\begin{figure}[H]
		\begin{tikzpicture}
		\begin{axis}[axis on top, 
		xmin=1,
		xmax=300,
		ymin=-50,
		ymax=100,
		height=\textheight*0.40,
		set layers=standard,
		cycle list name=train,
		grid=both,
		grid style={solid,gray!30!white},
		% axis lines=middle,
		xlabel=Episode,
		ylabel style={align=center}, ylabel=Reward Value,
		%legend style={at={(0.98,0.2)},anchor=east},
		legend pos=south east,
		extra y ticks = {90},
		extra y tick style={grid=major, grid style={solid,green},y tick label style={
				/pgf/number format/.cd,precision=10
		}},
		% x label style={at={(axis description cs:0.5,0)},anchor=north},
		%y label style={at={(axis description cs:-0.1,.5)},rotate=90,anchor=south}
		]
		
		\addplot table[x=Step,y=Value, col sep=comma] {plot/prioritized/mountain-v0/train/episode/mean.csv};
		\addlegendentry{Mean $\mu$ Reward Episode};
		
		\addplot [name path=upper,draw=none, forget plot] table[x=Step,y expr=\thisrow{Max}, col sep=comma] {plot/prioritized/mountain-v0/train/episode/mean.csv};
		
		\addplot [name path=lower,draw=none, forget plot] table[x=Step,y expr=\thisrow{Min}, col sep=comma] {plot/prioritized/mountain-v0/train/episode/mean.csv};
		
		\addplot [fill=train_color_3] fill between[of=upper and lower];
		\addlegendentry{Area $[min, max]$};
		
		\addplot [name path=upper1,draw=none, area legend, forget plot] table[x=Step,y expr=\thisrow{Value}+\thisrow{Error}, col sep=comma] {plot/prioritized/mountain-v0/train/episode/mean.csv};
		
		\addplot [name path=lower1,draw=none, area legend, forget plot] table[x=Step,y expr=\thisrow{Value}-\thisrow{Error}, col sep=comma] {plot/prioritized/mountain-v0/train/episode/mean.csv};
		
		\addplot [fill=train_color_2] fill between[of=upper1 and lower1];
		\addlegendentry{Area $[\mu-\sigma, \mu+\sigma]$};
		
		
		
		\end{axis}
		\end{tikzpicture}
		\caption{Mean, Standard Deviation Range and Min-Max range of the reward of each episode over 20 runs. }
		\label{fig:mountain_p_train_episode}
	%\end{figure}
	%\begin{figure}[h]
		\begin{tikzpicture}
		\begin{axis}[axis on top, 
		xmin=1,
		xmax=300,
		ymin=-50,
		ymax=100,
		height=\textheight*0.40,
		set layers=standard,
		cycle list name=train,
		grid=both,
		grid style={solid,gray!30!white},
		% axis lines=middle,
		xlabel=Episode,
		ylabel style={align=center}, ylabel=Reward Value,
		%legend style={at={(0.98,0.2)},anchor=east},
		legend pos=south east,
		extra y ticks = {90},
		extra y tick style={grid=major, grid style={solid,green},y tick label style={
				/pgf/number format/.cd,precision=10
		}},
		% x label style={at={(axis description cs:0.5,0)},anchor=north},
		%y label style={at={(axis description cs:-0.1,.5)},rotate=90,anchor=south}
		]
		
		\addplot table[x=Step,y=Value, col sep=comma] {plot/prioritized/mountain-v0/train/meanLast100/mean.csv};
		%\addlegendentry{Mean Reward of last 100 episode};
		
		\addplot [name path=upper,draw=none, forget plot] table[x=Step,y expr=\thisrow{Max}, col sep=comma] {plot/prioritized/mountain-v0/train/meanLast100/mean.csv};
		
		\addplot [name path=lower,draw=none, forget plot] table[x=Step,y expr=\thisrow{Min}, col sep=comma] {plot/prioritized/mountain-v0/train/meanLast100/mean.csv};
		
		\addplot [fill=train_color_3] fill between[of=upper and lower];
		
		\addplot [name path=upper1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Value}+\thisrow{Error}, col sep=comma] {plot/prioritized/mountain-v0/train/meanLast100/mean.csv};
		
		\addplot [name path=lower1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Value}-\thisrow{Error}, col sep=comma] {plot/prioritized/mountain-v0/train/meanLast100/mean.csv};
		
		\addplot [fill=train_color_2] fill between[of=upper1 and lower1];
		
		\addlegendentry{Mean $\mu$ Reward Last 100};
		\addlegendentry{Area $[min, max]$};
		\addlegendentry{Area $[\mu-\sigma, \mu+\sigma]$};
		\end{axis}
		
		\end{tikzpicture}
		\caption{Mean, Standard Deviation Range and Min-Max range of the running reward mean of the last 100 episodes for each episode over 20 runs.}
		\label{fig:mountain_p_train_mean}
		\end{figure}
		\begin{figure}[h]
		\begin{tikzpicture}
		\begin{axis}[axis on top, 
		xmin=10,
		xmax=300,
		ymin=-50,
		ymax=100,
		height=\textheight*0.4,
		set layers=standard,
		cycle list name=test,
		grid=both,
		grid style={solid,gray!30!white},
		% axis lines=middle,
		xlabel=Episode,
		ylabel style={align=center}, ylabel=Reward Value,
		%legend style={at={(0.98,0.2)},anchor=east},
		legend pos=south east,
		extra y ticks = {90},
		extra y tick style={grid=major, grid style={solid,green},y tick label style={
				/pgf/number format/.cd,precision=10
		}},
		% x label style={at={(axis description cs:0.5,0)},anchor=north},
		%y label style={at={(axis description cs:-0.1,.5)},rotate=90,anchor=south}
		]
		
		\addplot table[x=Step,y=Value, col sep=comma, fill=test_color_1] {plot/prioritized/mountain-v0/test/rewardMean/mean.csv};
		%\addlegendentry{Mean Reward of last 100 episode};
		
		\addplot [name path=upper,draw=none, forget plot] table[x=Step,y expr=\thisrow{Max}, col sep=comma] {plot/prioritized/mountain-v0/test/rewardMean/mean.csv};
		
		\addplot [name path=lower,draw=none, forget plot] table[x=Step,y expr=\thisrow{Min}, col sep=comma] {plot/prioritized/mountain-v0/test/rewardMean/mean.csv};
		
		\addplot [fill=test_color_3] fill between[of=upper and lower];
		
		\addplot [name path=upper1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Value}+\thisrow{Error}, col sep=comma] {plot/prioritized/mountain-v0/test/rewardMean/mean.csv};
		
		\addplot [name path=lower1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Value}-\thisrow{Error}, col sep=comma] {plot/prioritized/mountain-v0/test/rewardMean/mean.csv};
		
		\addplot [fill=test_color_2] fill between[of=upper1 and lower1];
		
		\addlegendentry{Mean $\mu$ Test Reward};
		\addlegendentry{Area $[min, max]$};
		\addlegendentry{Area $[\mu-\sigma, \mu+\sigma]$};
		
		
		\end{axis}
		\end{tikzpicture}
		\caption{Mean, Standard Deviation Range and Min-Max range of reward mean of the test phase (every 10 episodes) over 20 runs.}
		\label{fig:mountain_p_test}
	\end{figure}
	\begin{figure}[!h]
		\begin{tikzpicture}
		\begin{axis}[axis on top, 
		xmin=10,
		xmax=300,
		ymin=0,
		ymax=1000,
		height=\textheight*0.45,
		set layers=standard,
		cycle list name=test,
		grid=both,
		grid style={solid,gray!30!white},
		% axis lines=middle,
		xlabel=Episode,
		ylabel style={align=center}, ylabel=Reward Value,
		%legend style={at={(0.98,0.2)},anchor=east},
		legend pos=north east,
		%extra y ticks = {90},
		%extra y tick style={grid=major, grid style={solid,green},y tick label style={
		%		/pgf/number format/.cd,precision=10
		%}},
		% x label style={at={(axis description cs:0.5,0)},anchor=north},
		%y label style={at={(axis description cs:-0.1,.5)},rotate=90,anchor=south}
		]
		
		\addplot table[x=Step,y=Value, col sep=comma, fill=test_color_1] {plot/prioritized/mountain-v0/test/steps/mean.csv};
		%\addlegendentry{Mean Reward of last 100 episode};
		
		\addplot [name path=upper,draw=none, forget plot] table[x=Step,y expr=\thisrow{Max}, col sep=comma] {plot/prioritized/mountain-v0/test/steps/mean.csv};
		
		\addplot [name path=lower,draw=none, forget plot] table[x=Step,y expr=\thisrow{Min}, col sep=comma] {plot/prioritized/mountain-v0/test/steps/mean.csv};
		
		\addplot [fill=test_color_3] fill between[of=upper and lower];
		
		\addplot [name path=upper1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Value}+\thisrow{Error}, col sep=comma] {plot/prioritized/mountain-v0/test/steps/mean.csv};
		
		\addplot [name path=lower1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Value}-\thisrow{Error}, col sep=comma] {plot/prioritized/mountain-v0/test/steps/mean.csv};
		
		\addplot [fill=test_color_2] fill between[of=upper1 and lower1];
		
		\addlegendentry{Mean $\mu$ Test Step};
		\addlegendentry{Area $[min, max]$};
		\addlegendentry{Area $[\mu-\sigma, \mu+\sigma]$};
		
		
		\end{axis}
		\end{tikzpicture}
		\caption{Mean, Standard Deviation Range and Min-Max range of the number of steps in the test phase (every 10 episodes) over 20 runs.}
		\label{fig:mountain_p_test_step}
	\end{figure}
	\FloatBarrier
	
	\subsection{Pendulum-v0}
	In this case the graph about the steps taken in the test phase is not useful, because all episodes took all 200 steps.
	\subsubsection{DDPG with Uniform Replay Memory} 
	\begin{figure}[H]
		\begin{tikzpicture}
		\begin{axis}[axis on top, 
		xmin=1,
		xmax=200,
		ymin=-1800,
		ymax=0,
		height=\textheight*0.37,
		set layers=standard,
		cycle list name=train,
		grid=both,
		grid style={solid,gray!30!white},
		% axis lines=middle,
		xlabel=Episode,
		ylabel style={align=center}, ylabel=Reward Value,
		%legend style={at={(0.99,0.3)},anchor=east},
		legend pos=south east,
		% extra y ticks = {90},
	% 	extra y tick style={grid=major, grid style={solid,green},y tick label style={
		% 		/pgf/number format/.cd,precision=10
		%}},
		% x label style={at={(axis description cs:0.5,0)},anchor=north},
		%y label style={at={(axis description cs:-0.1,.5)},rotate=90,anchor=south}
		]
		
		\addplot table[x=Step,y=Value, col sep=comma] {plot/replayBuffer/pendulum-v0/train/episode/mean.csv};
		%\addlegendentry{Mean Reward of last 100 episode};
		
		\addplot [name path=upper,draw=none, forget plot] table[x=Step,y expr=\thisrow{Max}, col sep=comma] {plot/replayBuffer/pendulum-v0/train/episode/mean.csv};
		
		\addplot [name path=lower,draw=none, forget plot] table[x=Step,y expr=\thisrow{Min}, col sep=comma] {plot/replayBuffer/pendulum-v0/train/episode/mean.csv};
		
		\addplot [fill=train_color_3] fill between[of=upper and lower];
		
		\addplot [name path=upper1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Value}+\thisrow{Error}, col sep=comma] {plot/replayBuffer/pendulum-v0/train/episode/mean.csv};
		
		\addplot [name path=lower1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Value}-\thisrow{Error}, col sep=comma] {plot/replayBuffer/pendulum-v0/train/episode/mean.csv};
		
		\addplot [fill=train_color_2] fill between[of=upper1 and lower1];
		
		\addlegendentry{Mean $\mu$};
		\addlegendentry{Area $[min, max]$};
		\addlegendentry{Area $[\mu-\sigma, \mu+\sigma]$};
		
		\end{axis}
		\end{tikzpicture}
		\caption{Mean, Standard Deviation Range and Min-Max range of the reward of each episode over 10 runs.}
		\label{fig:pendulum_r_train_episode}
	%\end{figure}
	%\begin{figure}[H]
		\begin{tikzpicture}
		\begin{axis}[axis on top, 
		xmin=1,
		xmax=200,
		ymin=-1600,
		ymax=0,
		height=\textheight*0.37,
		set layers=standard,
		cycle list name=train,
		grid=both,
		grid style={solid,gray!30!white},
		% axis lines=middle,
		xlabel=Episode,
		ylabel style={align=center}, ylabel=Reward Value,
		%legend style={at={(0.99,0.3)},anchor=east},
		legend pos=south east,
		% extra y ticks = {90},
		% 	extra y tick style={grid=major, grid style={solid,green},y tick label style={
		% 		/pgf/number format/.cd,precision=10
		%}},
		% x label style={at={(axis description cs:0.5,0)},anchor=north},
		%y label style={at={(axis description cs:-0.1,.5)},rotate=90,anchor=south}
		]
		
		\addplot table[x=Step,y=Value, col sep=comma] {plot/replayBuffer/pendulum-v0/train/meanLast100/mean.csv};
		%\addlegendentry{Mean Reward of last 100 episode};
		
		\addplot [name path=upper,draw=none, forget plot] table[x=Step,y expr=\thisrow{Max}, col sep=comma] {plot/replayBuffer/pendulum-v0/train/meanLast100/mean.csv};
		
		\addplot [name path=lower,draw=none, forget plot] table[x=Step,y expr=\thisrow{Min}, col sep=comma] {plot/replayBuffer/pendulum-v0/train/meanLast100/mean.csv};
		
		\addplot [fill=train_color_3] fill between[of=upper and lower];
		
		\addplot [name path=upper1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Value}+\thisrow{Error}, col sep=comma] {plot/replayBuffer/pendulum-v0/train/meanLast100/mean.csv};
		
		\addplot [name path=lower1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Value}-\thisrow{Error}, col sep=comma] {plot/replayBuffer/pendulum-v0/train/meanLast100/mean.csv};
		
		\addplot [fill=train_color_2] fill between[of=upper1 and lower1];
		
		\addlegendentry{Mean $\mu$};
		\addlegendentry{Area $[min, max]$};
		\addlegendentry{Area $[\mu-\sigma, \mu+\sigma]$};
		
		\end{axis}
		\end{tikzpicture}
		\caption{Mean, Standard Deviation Range and Min-Max range of the running reward mean of the last 100 episodes for each episode over 10 runs.}
		\label{fig:pendulum_r_train_mean}
	\end{figure}
	\begin{figure}[H]
		\begin{tikzpicture}
		\begin{axis}[axis on top, 
		xmin=20,
		xmax=200,
		ymin=-1800,
		ymax=0,
		height=\textheight*0.40,
		set layers=standard,
		cycle list name=test,
		grid=both,
		grid style={solid,gray!30!white},
		% axis lines=middle,
		xlabel=Episode,
		ylabel style={align=center}, ylabel=Reward Value,
		%legend style={at={(0.99,0.3)},anchor=east},
		legend pos=south east,
		% extra y ticks = {90},
		% 	extra y tick style={grid=major, grid style={solid,green},y tick label style={
		% 		/pgf/number format/.cd,precision=10
		%}},
		% x label style={at={(axis description cs:0.5,0)},anchor=north},
		%y label style={at={(axis description cs:-0.1,.5)},rotate=90,anchor=south}
		]
		
		\addplot table[x=Step,y=Value, col sep=comma, fill=test_color_1] {plot/replayBuffer/pendulum-v0/test/rewardMean/mean.csv};
		%\addlegendentry{Mean Reward of last 100 episode};
		
		\addplot [name path=upper,draw=none, forget plot] table[x=Step,y expr=\thisrow{Max}, col sep=comma] {plot/replayBuffer/pendulum-v0/test/rewardMean/mean.csv};
		
		\addplot [name path=lower,draw=none, forget plot] table[x=Step,y expr=\thisrow{Min}, col sep=comma] {plot/replayBuffer/pendulum-v0/test/rewardMean/mean.csv};
		
		\addplot [fill=test_color_3] fill between[of=upper and lower];
		
		\addplot [name path=upper1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Value}+\thisrow{Error}, col sep=comma] {plot/replayBuffer/pendulum-v0/test/rewardMean/mean.csv};
		
		\addplot [name path=lower1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Value}-\thisrow{Error}, col sep=comma] {plot/replayBuffer/pendulum-v0/test/rewardMean/mean.csv};
		
		\addplot [fill=test_color_2] fill between[of=upper1 and lower1];
		
		\addlegendentry{Mean $\mu$};
		\addlegendentry{Area $[min, max]$};
		\addlegendentry{Area $[\mu-\sigma, \mu+\sigma]$};
		
		\end{axis}
		\end{tikzpicture}
		\caption{Mean, Standard Deviation Range and Min-Max range of reward mean of the test phase (every 10 episodes) over 10 runs.}
		\label{fig:pendulum_r_test}
	\end{figure}
	
	\FloatBarrier
	\subsubsection{SAC} \label{prioritized}
	\begin{figure}[!h]
		\begin{tikzpicture}
		\begin{axis}[axis on top, 
		xmin=1,
		xmax=200,
		ymin=-1800,
		ymax=0,
		height=\textheight*0.42,
		set layers=standard,
		cycle list name=train,
		grid=both,
		grid style={solid,gray!30!white},
		% axis lines=middle,
		xlabel=Episode,
		ylabel style={align=center}, ylabel=Reward Value,
		%legend style={at={(0.99,0.3)},anchor=east},
		legend pos=south east,
		% extra y ticks = {90},
		% 	extra y tick style={grid=major, grid style={solid,green},y tick label style={
		% 		/pgf/number format/.cd,precision=10
		%}},
		% x label style={at={(axis description cs:0.5,0)},anchor=north},
		%y label style={at={(axis description cs:-0.1,.5)},rotate=90,anchor=south}
		]
		
		\addplot table[x=Step,y=Value, col sep=comma] {plot/SAC/pendulum-v0/train/episode/mean.csv};
		%\addlegendentry{Mean Reward of last 100 episode};
		
		\addplot [name path=upper,draw=none, forget plot] table[x=Step,y expr=\thisrow{Max}, col sep=comma] {plot/SAC/pendulum-v0/train/episode/mean.csv};
		
		\addplot [name path=lower,draw=none, forget plot] table[x=Step,y expr=\thisrow{Min}, col sep=comma] {plot/SAC/pendulum-v0/train/episode/mean.csv};
		
		\addplot [fill=train_color_3] fill between[of=upper and lower];
		
		\addplot [name path=upper1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Value}+\thisrow{Error}, col sep=comma] {plot/SAC/pendulum-v0/train/episode/mean.csv};
		
		\addplot [name path=lower1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Value}-\thisrow{Error}, col sep=comma] {plot/SAC/pendulum-v0/train/episode/mean.csv};
		
		\addplot [fill=train_color_2] fill between[of=upper1 and lower1];
		
		\addlegendentry{Mean $\mu$};
		\addlegendentry{Area $[min, max]$};
		\addlegendentry{Area $[\mu-\sigma, \mu+\sigma]$};
		
		\end{axis}
		\end{tikzpicture}
		\caption{Mean, Standard Deviation Range and Min-Max range of the reward of each episode over 10 runs.}
		\label{fig:pendulum_p_train_episode}
	\end{figure}
	\begin{figure}[!h]
		\begin{tikzpicture}
		\begin{axis}[axis on top, 
		xmin=1,
		xmax=200,
		ymin=-1600,
		ymax=0,
		height=\textheight*0.42,
		set layers=standard,
		cycle list name=train,
		grid=both,
		grid style={solid,gray!30!white},
		% axis lines=middle,
		xlabel=Episode,
		ylabel style={align=center}, ylabel=Reward Value,
		%legend style={at={(0.99,0.3)},anchor=east},
		legend pos=south east,
		% extra y ticks = {90},
		% 	extra y tick style={grid=major, grid style={solid,green},y tick label style={
		% 		/pgf/number format/.cd,precision=10
		%}},
		% x label style={at={(axis description cs:0.5,0)},anchor=north},
		%y label style={at={(axis description cs:-0.1,.5)},rotate=90,anchor=south}
		]
		
		\addplot table[x=Step,y=Value, col sep=comma] {plot/SAC/pendulum-v0/train/meanLast100/mean.csv};
		%\addlegendentry{Mean Reward of last 100 episode};
		
		\addplot [name path=upper,draw=none, forget plot] table[x=Step,y expr=\thisrow{Max}, col sep=comma] {plot/SAC/pendulum-v0/train/meanLast100/mean.csv};
		
		\addplot [name path=lower,draw=none, forget plot] table[x=Step,y expr=\thisrow{Min}, col sep=comma] {plot/SAC/pendulum-v0/train/meanLast100/mean.csv};
		
		\addplot [fill=train_color_3] fill between[of=upper and lower];
		
		\addplot [name path=upper1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Value}+\thisrow{Error}, col sep=comma] {plot/SAC/pendulum-v0/train/meanLast100/mean.csv};
		
		\addplot [name path=lower1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Value}-\thisrow{Error}, col sep=comma] {plot/SAC/pendulum-v0/train/meanLast100/mean.csv};
		
		\addplot [fill=train_color_2] fill between[of=upper1 and lower1];
		
		\addlegendentry{Mean $\mu$};
		\addlegendentry{Area $[min, max]$};
		\addlegendentry{Area $[\mu-\sigma, \mu+\sigma]$};
		
		\end{axis}
		\end{tikzpicture}
		\caption{Mean, Standard Deviation Range and Min-Max range of the running reward mean of the last 100 episodes for each episode over 10 runs.}
		\label{fig:pendulum_p_train_mean}
	\end{figure}
\vfill
	\begin{figure}[!h]
		\begin{tikzpicture}
		\begin{axis}[axis on top, 
		xmin=20,
		xmax=200,
		ymin=-1800,
		ymax=0,
		height=\textheight*0.42,
		set layers=standard,
		cycle list name=test,
		grid=both,
		grid style={solid,gray!30!white},
		% axis lines=middle,
		xlabel=Episode,
		ylabel style={align=center}, ylabel=Reward Value,
		%legend style={at={(0.99,0.3)},anchor=east},
		legend pos=south east,
		% extra y ticks = {90},
		% 	extra y tick style={grid=major, grid style={solid,green},y tick label style={
		% 		/pgf/number format/.cd,precision=10
		%}},
		% x label style={at={(axis description cs:0.5,0)},anchor=north},
		%y label style={at={(axis description cs:-0.1,.5)},rotate=90,anchor=south}
		]
		
		\addplot table[x=Step,y=Value, col sep=comma, fill=test_color_1] {plot/SAC/pendulum-v0/test/rewardMean/mean.csv};
		%\addlegendentry{Mean Reward of last 100 episode};
		
		\addplot [name path=upper,draw=none, forget plot] table[x=Step,y expr=\thisrow{Max}, col sep=comma] {plot/SAC/pendulum-v0/test/rewardMean/mean.csv};
		
		\addplot [name path=lower,draw=none, forget plot] table[x=Step,y expr=\thisrow{Min}, col sep=comma] {plot/SAC/pendulum-v0/test/rewardMean/mean.csv};
		
		\addplot [fill=test_color_3] fill between[of=upper and lower];
		
		\addplot [name path=upper1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Value}+\thisrow{Error}, col sep=comma] {plot/SAC/pendulum-v0/test/rewardMean/mean.csv};
		
		\addplot [name path=lower1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Value}-\thisrow{Error}, col sep=comma] {plot/SAC/pendulum-v0/test/rewardMean/mean.csv};
		
		\addplot [fill=test_color_2] fill between[of=upper1 and lower1];
		
		\addlegendentry{Mean $\mu$};
		\addlegendentry{Area $[min, max]$};
		\addlegendentry{Area $[\mu-\sigma, \mu+\sigma]$};
		
		\end{axis}
		\end{tikzpicture}
		\caption{Mean, Standard Deviation Range and Min-Max range of reward mean of the test phase (every 20 episodes) over 10 runs.}
		\label{fig:pendulum_p_test}
	\end{figure}
	\FloatBarrier
	
	\subsubsection{SAC with Autotune} \label{prioritized}
	\begin{figure}[!h]
		\begin{tikzpicture}
		\begin{axis}[axis on top, 
		xmin=1,
		xmax=200,
		ymin=-1800,
		ymax=0,
		height=\textheight*0.42,
		set layers=standard,
		cycle list name=train,
		grid=both,
		grid style={solid,gray!30!white},
		% axis lines=middle,
		xlabel=Episode,
		ylabel style={align=center}, ylabel=Reward Value,
		%legend style={at={(0.99,0.3)},anchor=east},
		legend pos=south east,
		% extra y ticks = {90},
		% 	extra y tick style={grid=major, grid style={solid,green},y tick label style={
		% 		/pgf/number format/.cd,precision=10
		%}},
		% x label style={at={(axis description cs:0.5,0)},anchor=north},
		%y label style={at={(axis description cs:-0.1,.5)},rotate=90,anchor=south}
		]
		
		\addplot table[x=Step,y=Value, col sep=comma] {plot/SACAT/pendulum-v0/train/episode/mean.csv};
		%\addlegendentry{Mean Reward of last 100 episode};
		
		\addplot [name path=upper,draw=none, forget plot] table[x=Step,y expr=\thisrow{Max}, col sep=comma] {plot/SACAT/pendulum-v0/train/episode/mean.csv};
		
		\addplot [name path=lower,draw=none, forget plot] table[x=Step,y expr=\thisrow{Min}, col sep=comma] {plot/SACAT/pendulum-v0/train/episode/mean.csv};
		
		\addplot [fill=train_color_3] fill between[of=upper and lower];
		
		\addplot [name path=upper1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Value}+\thisrow{Error}, col sep=comma] {plot/SACAT/pendulum-v0/train/episode/mean.csv};
		
		\addplot [name path=lower1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Value}-\thisrow{Error}, col sep=comma] {plot/SACAT/pendulum-v0/train/episode/mean.csv};
		
		\addplot [fill=train_color_2] fill between[of=upper1 and lower1];
		
		\addlegendentry{Mean $\mu$};
		\addlegendentry{Area $[min, max]$};
		\addlegendentry{Area $[\mu-\sigma, \mu+\sigma]$};
		
		\end{axis}
		\end{tikzpicture}
		\caption{Mean, Standard Deviation Range and Min-Max range of the reward of each episode over 10 runs.}
		\label{fig:pendulum_p_train_episode}
	\end{figure}
	\begin{figure}[!h]
		\begin{tikzpicture}
		\begin{axis}[axis on top, 
		xmin=1,
		xmax=200,
		ymin=-1600,
		ymax=0,
		height=\textheight*0.42,
		set layers=standard,
		cycle list name=train,
		grid=both,
		grid style={solid,gray!30!white},
		% axis lines=middle,
		xlabel=Episode,
		ylabel style={align=center}, ylabel=Reward Value,
		%legend style={at={(0.99,0.3)},anchor=east},
		legend pos=south east,
		% extra y ticks = {90},
		% 	extra y tick style={grid=major, grid style={solid,green},y tick label style={
		% 		/pgf/number format/.cd,precision=10
		%}},
		% x label style={at={(axis description cs:0.5,0)},anchor=north},
		%y label style={at={(axis description cs:-0.1,.5)},rotate=90,anchor=south}
		]
		
		\addplot table[x=Step,y=Value, col sep=comma] {plot/SACAT/pendulum-v0/train/meanLast100/mean.csv};
		%\addlegendentry{Mean Reward of last 100 episode};
		
		\addplot [name path=upper,draw=none, forget plot] table[x=Step,y expr=\thisrow{Max}, col sep=comma] {plot/SACAT/pendulum-v0/train/meanLast100/mean.csv};
		
		\addplot [name path=lower,draw=none, forget plot] table[x=Step,y expr=\thisrow{Min}, col sep=comma] {plot/SACAT/pendulum-v0/train/meanLast100/mean.csv};
		
		\addplot [fill=train_color_3] fill between[of=upper and lower];
		
		\addplot [name path=upper1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Value}+\thisrow{Error}, col sep=comma] {plot/SACAT/pendulum-v0/train/meanLast100/mean.csv};
		
		\addplot [name path=lower1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Value}-\thisrow{Error}, col sep=comma] {plot/SACAT/pendulum-v0/train/meanLast100/mean.csv};
		
		\addplot [fill=train_color_2] fill between[of=upper1 and lower1];
		
		\addlegendentry{Mean $\mu$};
		\addlegendentry{Area $[min, max]$};
		\addlegendentry{Area $[\mu-\sigma, \mu+\sigma]$};
		
		\end{axis}
		\end{tikzpicture}
		\caption{Mean, Standard Deviation Range and Min-Max range of the running reward mean of the last 100 episodes for each episode over 10 runs.}
		\label{fig:pendulum_p_train_mean}
	\end{figure}
	\vfill
	\begin{figure}[!h]
		\begin{tikzpicture}
		\begin{axis}[axis on top, 
		xmin=20,
		xmax=200,
		ymin=-1800,
		ymax=0,
		height=\textheight*0.42,
		set layers=standard,
		cycle list name=test,
		grid=both,
		grid style={solid,gray!30!white},
		% axis lines=middle,
		xlabel=Episode,
		ylabel style={align=center}, ylabel=Reward Value,
		%legend style={at={(0.99,0.3)},anchor=east},
		legend pos=south east,
		% extra y ticks = {90},
		% 	extra y tick style={grid=major, grid style={solid,green},y tick label style={
		% 		/pgf/number format/.cd,precision=10
		%}},
		% x label style={at={(axis description cs:0.5,0)},anchor=north},
		%y label style={at={(axis description cs:-0.1,.5)},rotate=90,anchor=south}
		]
		
		\addplot table[x=Step,y=Value, col sep=comma, fill=test_color_1] {plot/SACAT/pendulum-v0/test/rewardMean/mean.csv};
		%\addlegendentry{Mean Reward of last 100 episode};
		
		\addplot [name path=upper,draw=none, forget plot] table[x=Step,y expr=\thisrow{Max}, col sep=comma] {plot/SACAT/pendulum-v0/test/rewardMean/mean.csv};
		
		\addplot [name path=lower,draw=none, forget plot] table[x=Step,y expr=\thisrow{Min}, col sep=comma] {plot/SACAT/pendulum-v0/test/rewardMean/mean.csv};
		
		\addplot [fill=test_color_3] fill between[of=upper and lower];
		
		\addplot [name path=upper1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Value}+\thisrow{Error}, col sep=comma] {plot/SACAT/pendulum-v0/test/rewardMean/mean.csv};
		
		\addplot [name path=lower1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Value}-\thisrow{Error}, col sep=comma] {plot/SACAT/pendulum-v0/test/rewardMean/mean.csv};
		
		\addplot [fill=test_color_2] fill between[of=upper1 and lower1];
		
		\addlegendentry{Mean $\mu$};
		\addlegendentry{Area $[min, max]$};
		\addlegendentry{Area $[\mu-\sigma, \mu+\sigma]$};
		
		\end{axis}
		\end{tikzpicture}
		\caption{Mean, Standard Deviation Range and Min-Max range of reward mean of the test phase (every 20 episodes) over 10 runs.}
		\label{fig:pendulum_p_test}
	\end{figure}
	\FloatBarrier
	
	\newpage
	\section{Comments} \label{final}
	
	Analyzing the graphs, it is clear that the results in moving from the Replay Buffer to the Prioritized Buffer are not as impressive as it was expected.
	
	The results obtained are almost the same or slightly better, on the other hand the execution time increases: for this reason the number of element extracted from the mini-batch at each episode was decreased.
	
	In the \textit{MountainCarContinuous-v0} environment, the mean on the last 100 episodes shown in \vref{fig:mountain_p_train_mean} is not better than \vref{fig:mountain_r_train_mean}, especially around 150th episode. But above the 260th episode the results are more stable and efficient.
	Also comparing the graphs of test phase, the results do not strongly improve.
	
	In the \textit{Pendulum-v0} environment the situation is overall the same, without any important improvement. In some cases the Uniform Replay Buffer seems more efficient in the first part, while the Prioritized one seems more stable in the last episodes.
	
	The aim of the Prioritized Buffer is to give more importance to surprising results. For this reason, I think that having worse results in the first part of the training (0-100 episodes) is normal and it is part of the algorithm. I expected impressive results in the last part, but unfortunately I obtained only slightly better results.
	
	I think that the problem may be caused by some bugs in the implementation of this part.
		
	\section{Next Steps} \label{next}
	The next steps that I am planning to analyze are:
	\begin{description}
		\item[Prioritized Problems] I think that the problem behind not impressive results of Prioritized Buffer are related to some bug in the implementation of this part. I will try to find a way to fix this.
		\item[Convolutional Neural Network] I had already started to test this part, but I have no relevant results, yet. I applied  the code to \textit{CarRacing-v0} which propose a continuous environment with a RGB vector as observation. I used a \texttt{StateBuffer} of size three to use the last three states as input for the Convolutional Neural Network ($9 \text{ inputs} = 3 \cdot 3 \text{ RGB channels}$).
		
		I found difficulties to test whether an algorithm or a particular CNN architecture is working because of the length of the training in this particular environment. I will try to find a better way to test: an idea could be to apply this approach to \textit{MountainCarConinuous-v0} or \textit{Pendulum-v0}, trying to get the image as observation instead of raw data.
		
		This is the \textbf{most important part} to develop because it is the base of the work with Anki Cozmo. I will focus mainly on that in the next days.
		
		\item[DDPG vs others] I am thinking to invest a part of the work in searching alternative algorithms recently discovered and reading new papers about them. For instance, some of the latest algorithm used in Autonomous Driving Reinforcement Learning are \textbf{Deep Distributed Distributional Deterministic Policy Gradients (D4PG)}, \textbf{Twin Delayed DDPG (TD3)} and \textbf{Soft Actor Critic (SAC)}. The last one is a sort of bridge between stochastic policy optimization and DDPG-style approaches. They seems promising in the context of the project and maybe they could lead to better results.
		I will try to explore more deeply this part.
		
	\end{description}	
	\newpage
	
	\bibliographystyle{unsrt}
	\bibliography{references}
	
\end{document}
