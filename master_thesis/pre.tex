\errorcontextlines=9
\english
\iflanguage{english}{%
	\retrofrontespizio{This work is subject to the Creative Commons Licence}
	% \DottoratoIn{PhD Course in\space}
	\StrutturaDidattica{Department of }
	\struttura{Control and Computer Engineering}
	\CorsoDiLaureaIn{Master of Science in\space}
	%\NomeMonografia{Bachelor Degree Final Work}
	\TesiDiLaurea{Master Thesis}
	%\NomeDissertazione{PhD Dissertation}
	%\InName{in}
	\CandidateName{Candidate}% or Candidate
	\AdvisorName{Supervisors}% or Supervisor
	\TutorName{Tutor}
	\NomeTutoreAziendale{Internship Tutor}
	%\CycleName{cycle}
	%\NomePrimoTomo{First volume}
	%\NomeSecondoTomo{Second Volume}
	%\NomeTerzoTomo{Third Volume}
	%\NomeQuartoTomo{Fourth Volume}
	%\logosede{logodue}% or comma separated list of logos
	\TitoloListaCandidati{Candidate,Candidate,Candidates,Candidates}
}{}
%%%%%%% Questi comandi è meglio metterli dentro l'ambiente
%%%%%%% frontespizio o frontespizio*, oppure in un file di
%%%%%%% configurazione personale. Si veda la documentazione
%%%%%%% inglese o italiana.
%%%%%%% Comunque i presenti comandi servono per comporre la
%%%%%%% tesi con i moduli di estensione standard del pacchetto
%%%%%%% TOPtesi.

\begin{ThesisTitlePage}
	\ateneo{Politecnico di Torino}
	\titolo{Deep Reinforcement Learning for autonomous systems}
	\sottotitolo{Designing a control system to exploit model-free deep reinforcement learning algorithms to solve a real-world autonomous driving task of a small robot.}
	%%%%%%% Corso degli studi
	\corsodilaurea{Computer Engineering (Software Career)}
	%%%%%%% L'eventuale numero di matricola va fra parentesi quadre
	\renewcommand\IDlabel{\\\quad\xspace}
	\candidato{Piero \textsc{Macaluso}}[s252894]
	%\secondocandidato{Evangelista \textsc{Torricelli}}[123457]

	%%%%%%% Relatori o supervisori
	%
	\relatore{Prof.~Pietro \textsc{Michiardi}}
	\secondorelatore{Prof.~Elena \textsc{Baralis}}

	%%%%%%% Seduta dell'esame
	%		\sedutadilaurea{\textsc{October} 2019}
	%%%%%%%% oppure:
	\sedutadilaurea{\textsc{Academic~Year} 2019-2020}% 
	%%%%%%% Logo della sede
	\logosede{logopolito}% 
\end{ThesisTitlePage}


%%%%%%% Per cambiare l'offset per la rilegatura;
%%%%%%% meno offset c'e', meglio e'
%\setbindingcorrection{3mm}

\begin{abstract}
	Because of its potential to thoroughly change mobility and transport, autonomous systems and self-driving vehicles are attracting much attention from both the research community and industry.
	Recent work has demonstrated that it is possible to rely on a comprehensive understanding of the immediate environment while following simple high-level directions, to obtain a more scalable approach that can make autonomous driving a ubiquitous technology.
	However, to date, the majority of the methods concentrates on deterministic control optimisation algorithms to select the right action, while the usage of deep learning and machine learning is entirely dedicated to object detection and recognition.

	Recently, we have witnessed a remarkable increase in interest in Reinforcement Learning (RL). It is a machine learning field focused on solving Markov Decision Processes (MDP), where an agent learns to make decisions by mapping situations and actions according to the information it gathers from the surrounding environment and from the reward it receives, trying to maximise it.
	As researchers discovered, it can be surprisingly useful to solve tasks in simulated environments like games and computer games, and it showed encouraging performance in tasks with robotic manipulators. Furthermore, the great fervour produced by the widespread exploitation of deep learning opened the doors to function approximation with convolutional neural networks, developing what is nowadays known as deep reinforcement learning.

	In this thesis, we argue that the generality of reinforcement learning makes it a useful framework where to apply autonomous driving to inject artificial intelligence not only in the detection component but also in the decision-making one.
	The focus of the majority of reinforcement learning projects is on a simulated environment. However, a more challenging approach of reinforcement learning consists of the application of this type of algorithms in the real world.
	For this reason, we designed and implemented a control system for Cozmo, a small toy robot developed by Anki company, by exploiting the Cozmo SDK, PyTorch and OpenAI Gym to build up a standardised environment in which to apply any reinforcement learning algorithm: it represents the first contribution of our thesis.

	Furthermore, we designed a circuit where we were able to carry out experiments in the real world, the second contribution of our work.
	We started from a simplified environment where to test algorithm functionalities to motivate and discuss our implementation choices.
	Therefore, we implemented our version of Soft Actor-Critic (SAC), a model-free reinforcement learning algorithm suitable for real-world experiments, to solve the specific self-driving task with Cozmo. The agent managed to reach a maximum value of above 3.5 meters in the testing phase, which equals more than one complete tour of the track. Despite this significant result, it was not able to learn how to drive securely and stably. Thus, we focused on the analysis of the strengths and weaknesses of this approach outlining what could be the next steps to make this cutting-edge technology concrete and efficient.

\end{abstract}

% \paginavuota % funziona anche senza specificare l'opzione classica

%\printglossaries

% \ringraziamenti

% \todomacaluso{Acknowledgements must be prepared!}

\tablespagetrue\figurespagetrue % normalmente questa riga non serve ed e' commentata
\indici

%%%%%%%% Altro esperimento con l'opzione classica
%%%%%%%% Non usare mai anche se qui lo si è fatto!
%%%%%%%% Oltretutto funziona solo se si è specificata la lingua greca fra le opzioni.
%%%%%%%% Commentare fra \ifclassica fino a \fi compresi. 
%\ifclassica
%	\begin{citazioni}
%		\textit{testo testo testo\\testo testo testo}
%
%		[\textsc{G.\ Leopardi}, Operette Morali]\vspace{1em}
%
%		\textgreek{>all'a p'anta <o k'eraunos d'' >oiak'izei}
%
%		[\textsc{Eraclito}, fr.\ D-K 134]
%	\end{citazioni}

%\fi
%%%%%%%% fine esperimento
