\chapter{Experimental results} \label{ch:ch5}

In the previous chapters, we described the reinforcement learning control system we designed, together with an analysis of the solutions we proposed for the problems we faced during the development process.
Indeed, this process has not been free from difficulties, both of implementation level and parameter optimisation.
After completing the design of this architecture, our second goal was to look for an algorithm that could better adapt to a real context, exceeding the limits set by DDPG.
The ideal would have been to find an algorithm altogether parameter agnostic: an enabling feature to achieve excellent performance regardless of the specific configuration.
During our research we came across the SAC algorithm and, after a careful analysis of the paper and having understood the considerations made by the authors about SAC real-world applications, we thought it might be the right choice to get better performance than the DDPG experiments.
For this reason, this chapter aims to present a detailed comparison of the experiments carried out on both algorithms.

The first section of this chapter focuses on the experimental methodology.
It will be an opportunity to describe the hardware of the development machine we used for the experiments and present in a more schematic and precise way the OpenAI Gym environments on which we will apply the algorithms.
We speak in the plural, because we have decided to report both the experiments performed on \textit{Pendulum-v0} environment, and those carried out with Anki Cozmo in the real world using the architecture we built.
This part will also contain a brief analysis of how reinforcement learning experiments are assessed to date.
For this segment, we took inspiration by \cite{henderson2018deep}, an exciting publication where the authors investigated reproducibility challenges, proper experimental techniques, and reporting procedures of modern deep reinforcement learning to draw up guidelines from which to start in order to obtain better reports, not so much from the result perspective, but from how they are reported.

The second and third sections of this chapter will, therefore, be devoted respectively to the two types of environments used.
We have shown all the useful graphs in order to analyse and to comment on the obtained results.

\section{Experimental Methodology}

This pre-trial section is essential to understand better the tasks we tried to get the reinforcement learning algorithms to solve and what approach we used to evaluate experiments results.

\subsection{Hardware and Software details}

In order to carry out the experiments contained in this chapter, we have made use of a personal laptop.
We chose this solution because the machine has excellent specifications to support the computational power required by the experiments both in terms of GPU and RAM.

Despite these considerations, we still had problems in terms of RAM memory. This type of experiment requires a very large experience memory replay to allow optimal batch extraction. Despite the large amount of RAM present and the reduction of the size of the input image, we were still forced to reduce the maximum size of the replay memory to complete the experiments.

We collected the essential information about hardware and software that we have used to perform experiments in \vref{table:hw_spec,table:sw_spec}.

\begin{table}[!h]
	\centering
	\caption{Development Machine Hardware Specifications}
	\label{table:hw_spec}
	\begin{tabular}{@{}lllll@{}}
		\toprule
		\textbf{Component} & \textbf{Details}                                                                      \\
		\midrule
		\textbf{Laptop}    & Dell Inspiron 15 7559                                                                 \\\midrule
		\textbf{CPU}       & Intel\textsuperscript{\textregistered} Core\textsuperscript{\texttrademark} i7-6700HQ \\
		                   & \# of Cores: 4                                                                        \\
		                   & \# of Threads: 8                                                                      \\
		                   & Processor Base Frequency: 2.60 GHz                                                    \\
		                   & Max Turbo Frequency: 3.50 GHz                                                         \\\midrule
		\textbf{GPU}       & NVIDIA GeForce GTX 960M                                                               \\
		                   & CUDA Cores: 640                                                                       \\
		                   & Memory: 4GB GDDR5, 2500 MHz                                                           \\\midrule
		\textbf{RAM}       & 12GB DDR3L, 1600 MHz                                                                  \\
		\bottomrule
	\end{tabular}
\end{table}

\begin{table}[!h]
	\centering
	\caption{Development Machine Software Specifications}
	\label{table:sw_spec}
	\begin{tabular}{@{}lllll@{}}
		\toprule
		\textbf{Component}        & \textbf{Details}                   \\
		\midrule
		\textbf{Operating System} & Ubuntu 18.04.3 LTS (Bionic Beaver) \\\midrule
		\textbf{Python}           & v.3.6.8                            \\\midrule
		\textbf{PyTorch}          & v.1.4.0                            \\\midrule
		\textbf{OpenAI Gym}       & v.0.15.4                           \\
		\bottomrule
	\end{tabular}
\end{table}

\subsection{Pendulum-v0 Environment}

OpenAI Gym \textit{Pendulum-v0} environment formalise the inverted pendulum swing-up problem, a classic problem in the control literature. In this version of the problem, the pendulum starts in a random position, and the goal is to swing it up so it stays upright.

We have also decided to include in this thesis the experiments we have carried out on this simulated environment, because the results obtained and the problems faced were essential to have a more prepared approach to deal with the real-world experiment and the environment we designed for Cozmo.

\begin{figure}[ht!]
	\centering
	\includegraphics[height=0.2\paperwidth]{img/pendulum.png}
	\caption[Frame of Pendulum-v0 environment]{Frame of Pendulum-v0 environment. We decided to use a set of two subsequent 64$\times$64 images.}
	\label{fig:pendulum}
\end{figure}

\subsubsection{Observation}

The original implementation of this environment is based on an state represented by a \texttt{Box(3)} type, a data structure defined by OpenAI Gym that extends functionalities of a standard array. It contains values related to the current angle of the pendulum as described in \vref{table:pendulum_obs}.

Since the goal of our thesis was to apply DDPG and SAC to a problem such as the autonomous driving one where input data is composed of images, we decided to build a wrapper (\texttt{gym.ObservationWrapper}) for the original environment in order to receive observations as raw pixels. In this way we could apply the same considerations and the same convolutional neural network that we used in the Cozmo environment.

We have started many experiments on this environment to find the most suitable number of images to use as a state, looking for a trade-off between the algorithmâ€™s needs and the space constraints imposed by the hardware we used. The agent revealed instability using a single frame, while leading to excellent results using two images. Finally, for space issues, we decided to resize the image to 64$\times$64 pixels.

A sample screenshot of the environment in action is shown in \vref{fig:pendulum}.

\begin{table}[!h]
	\centering
	\caption{Original Observation \texttt{Pendulum-v0} environment}
	\label{table:pendulum_obs}
	\begin{tabular}{@{}lllll@{}}
		\toprule
		Index & Observation    & Min    & Max    \\ \midrule
		0     & cos($\theta$)  & $-1.0$ & $+1.0$ \\
		1     & sin($\theta$)  & $-1.0$ & $+1.0$ \\
		2     & $\dot{\theta}$ & $-8.0$ & $+8.0$ \\
		\bottomrule
	\end{tabular}
\end{table}

\subsubsection{Actions}

The actions that the agent can perform within this environment are described through a \texttt{Box(1)} object containing only one element. This value corresponds to the joint effort, which allows the agent to swing the pendulum. The action space has been maintained also in the modified environment.

\begin{table}[!h]
	\centering
	\caption{Pendulum-v0 Actions }
	\label{mountain_action}
	\begin{tabular}{@{}lllll@{}}
		\toprule
		Index & Action       & Min    & Max    \\ \midrule
		0     & Joint effort & $-2.0$ & $+2.0$ \\
		\bottomrule
	\end{tabular}
\end{table}

\subsubsection{Reward}
The reward for each timestep $t$ is given by \[r_t = -(\theta_t^2 + 0.1 \dot{\theta}^2 + 0.001 a_t^2)\]
where theta is normalized between $-\pi$ and $\pi$. Therefore, the lowest cost is $-(\pi^2 + 0.1*8^2 + 0.001*2^2) = -16.2736044$, and the highest cost is $0$. In essence, the goal is to remain at zero angle (vertical), with the least rotational velocity, and the least effort.
The reward design has been maintained also in the modified environment.

\subsubsection{Starting State}

The initial state of the environment in question is chosen randomly.
Two values are extracted: the first is an angle between  $-\pi$ to $\pi$, the second is a speed between -1 and 1.
A zero angle corresponds to the standing pendulum.

\subsubsection{Episode Termination}

OpenAI Gym documentation does not specify a particular episode termination for this environment: the choice is left to the user.
In our case, after some attempts, we decided to set a limit value of 200 steps for each episode.

\subsubsection{Solved Requirements}

Also in this case, OpenAI Gym documentation does not specify any indications in order to understand whether an episode has been solved.
Indeed, \textit{Pendulum-v0} is an unsolved environment, which means it does not have a specified reward threshold at which it is considered correctly completed.

\subsection{CozmoEnv-v0 Environment}

\textit{CozmoEnv-v0}, the reinforcement learning environment we implemented, is one of the contribution of this thesis.
This section aims to present as schematically as possible the basic parameters that characterize the environment we have designed.
Further details on the implementation choices, problems encountered and solutions we have adopted to solve them are available in \vref{ch:ch4}.

\subsubsection{Observation}

The observations we decided to use in \textit{CozmoEnv-v0} are the same as those we exploited in the \textit{Pendulum-v0} environment.
In fact, our agent will obtain, for each action carried out, a queue composed of two images from the front camera of Cozmo resized to 64x64 pixels.
As in the previous environment, we decided to resize the images obtained in order to remain within the limits placed by the RAM memory available in the development machine.

The first image represents the state before the action, while the second represents the consequences of the action taken.
The number of images was set to two after performing some experiments on \textit{Pendulum-v0} environment that revealed instability in the use of a single image.
We decided to limit ourselves to two images, as we would increase the size of a single entry in replay memory by adding more of them.
This choice would require a counterbalance that would materialize in the decrease of the maximum size of replay memory.

As we mentioned in \vref{ch:ch3}, Anki Cozmo has a front camera inserted inside its tilting head and one forklift.
In order to obtain more valuable images for our experiments we decided to tilt the head as much as possible down and raise the forklift: in this way the image is focused as much as possible on the lane, leaving everything that could distract the learning process outside the view.

An example of two subsequent frame received by Cozmo is available in \vref{fig:cozmo_frames}.
\begin{figure}

	\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[height=0.25\paperwidth]{img/cozmo_frame_1.jpg}
	\end{minipage}
	\begin{minipage}[t]{0.5\linewidth}
		\centering
		\includegraphics[height=0.25\paperwidth]{img/cozmo_frame_2.jpg}
	\end{minipage}

	\caption[Example of two subsequent frame of CozmoEnv-v0]{An example of a two subsequent frame of CozmoEnv-v0 environment. We decided to resize the image returned by Cozmo to 64$\times$64 for memory reason: a higher resolution would lead to a further decrease in experience replay memory.}
	\label{fig:cozmo_frames}
\end{figure}

\subsubsection{Actions}

We have already discussed in \vref{subsubsec:mdp_form} the decisions taken to implement the management of actions within the environment that we have built.

To describe the actions that the agent can perform within this environment, we used a \texttt{Box(2)}. The first value of this object corresponds to the desired speed, while the second one represent the steering wheel position. \Vref{table:cozmo_actions} describes schematically this object.

\begin{table}[!h]
	\centering
	\caption{CozmoEnv-v0 Actions}
	\label{table:cozmo_actions}
	\begin{tabular}{@{}lllll@{}}
		\toprule
		Index & Action                        & Min    & Max    \\ \midrule
		0     & Desired Speed ($v$)           & $0.0$  & $+1.0$ \\
		1     & Steering Wheel Position ($w$) & $-1.0$ & $+1.0$ \\

		\bottomrule
	\end{tabular}
\end{table}

\subsubsection{Reward}

The reward we chose for our experiment was the second one provided by \vref{subsubsec:mdp_form}.
The decision has fallen on the \textit{Lane Distance Reward} because it describes with simplicity the final goal of the task, but above all because it allows the user to have a direct counterproof of the effectiveness of the algorithm, by matching the reward to the distance travelled.
\Vref{eq:reward_fun} reports the calculation that is executed to every timestep to calculate the reward to the carried out action.
$c$ is the time expressed in seconds between one action and the next one, imposed as system constant, while $v_t$ is the desired speed taken by the current action expressed in millimetres per second.

\begin{equation}
	\label{eq:reward_fun}
	r_t = v_t \cdot c
\end{equation}

\subsubsection{Starting State}

The starting state position of the system is not constant, but changes from episode to episode.
This approach was preferred over the one with a fixed starting position for two simple reasons:
\begin{itemize}
	\item Reduces the path that has to be traveled by the robot in order to be able to reposition, speeding up the experiments as it saves more battery.
	      In this approach, the robot is repositioned on the road closest to where the previous episode ended.
	\item It allows the agent to accumulate experiences that do not always refer to the same road segment.
	      With this methodology, the experiences will most always begin and end in different states, leading the agent to put more effort in generalization.
\end{itemize}

The episode starts as soon as the user receives the start signal.

\subsubsection{Episode Termination}

The episode ends when the robot goes off the road or reaches a a dangerous situation.
Even this time, the episode ends when the user receives the stop sign.

\subsubsection{Solved Requirements}

We have not provided a well-defined parameter to understand whether the task has been solved or not, because it depends on the path and the particular needs of the programmer.
Potentially the episode could last forever if the robot could learn to run an entire circuit.

No mechanism has been implemented to communicate to the robot that the episode ended in a positive way.
For this reason, we suggest to apply this environment to circuits and not paths where beginning and end do not coincide.

In our case, the route used is almost 3 meters long.
So we decided to use this value as a target to reach to determine the resolution of the task.

\subsection{Measuring Performance}

In recent years, we have witnessed the rapid growth of interest in deep reinforcement learning by the entire scientific community.
This growth has led to an increase in experiments and work on this subject, which are often easily available.
However, reproducing reinforcement learning experiments is not always as intuitive and straightforward as expected.
Often, the measurements reported by papers and studies of this kind are difficult to interpret due to the non-determinism inherent in most environments in which these algorithms are applied.
Without appropriate and meaningful metrics accompanied by standardization in presenting the results, it becomes difficult to determine conclusively the improvements made to the-state-of-the-art.

Going through the literature, it is noticeable that reinforcement learning algorithms are often evaluated by presenting tables and graphs showing cumulative average rewards or the maximum reward achieved on a pre-set number of timesteps.
But the combined features of environments and algorithms make these values typically inadequate for fair comparison.
This is due to the fact that there are numerous factors that come into play, such as seeds and trials that lead to different performances and that do not contribute to make clearer the actual performance of an algorithm.
However, when these are accompanied by confidence intervals, based on a fairly large number of attempts, then there are the premises to make decisions and formulate more informed considerations.

Once again, however, we have been forced to come to terms with reality.
We were able to produce as honest and specific an analysis as possible regarding experiments on \textit{Pendulum-v0}.
In this case, we could easily repeat the experiments 10 times for each algorithm, so that we could report graphs containing more useful information, such as confidence margins.

On the other hand, experiments with Cozmo took a much larger number of episodes before starting to show the first improvements.
We managed to maintain an average of just under 1000 episodes per day: this underlines how difficult it was to get to conclude even a single experiment.
For this experiment, we reported the results obtained without any confidence margin, but focusing on the best training results as opposed to the results obtained during the test.

\Vref{sec:pendulum-exp,sec:cozmo-exp} will show the most important results and graphs obtained from the respective experiments.
The final part of each section will be accompanied by a comment on the results obtained paying particular attention to the comparison between the two algorithms.

\section{Pendulum-v0 Experiments} \label{sec:pendulum-exp}

\subsection{DDPG Hyperparameters}

The hyper-parameters we exploited in this experiment are shown in \vref{table:ddpg_pendulum}.
The epsilon decay function is presented in \vref{eq:epsilon_decay} where $e$ is the current episode number. It is used to decrease the impact of the noise on the actions in function of the number of episode. When it reaches the $\epsilon_{\text{end}}$, it will become a constant.

\begin{table}[!h]
	\centering
	\caption{SAC Hyper-parameter setup for Pendulum-v0 environment}
	\label{table:ddpg_pendulum}
	\resizebox{0.9\textwidth}{!}{
		\begin{tabular}{@{}lllll@{}}
			\toprule
			\textbf{Hyper-parameters}             & \textbf{Value}                                                    \\
			\midrule
			\textbf{Policy Network}               & \textbf{Learning Rate}: $1 \times 10^{-4}$                        \\
			                                      & \textbf{Architecture}                                             \\
			                                      & 3 CONV Layer $3\times 3\times 16$, stride 2, padding 0            \\
			                                      & 2 FC Layer with hidden size = 256                                 \\
			                                      & 1 Output value                                                    \\\midrule
			\textbf{Q Network}                    & \textbf{Learning Rate}: $1 \times 10^{-4}$                        \\
			                                      & \textbf{Architecture}                                             \\
			                                      & 3 CONV Layer $3\times 3\times 16$, stride 2, padding 0            \\
			                                      & 2 FC Layer with hidden size = 256                                 \\
			                                      & 1 Output value                                                    \\\midrule
			\textbf{Ornstein Uhlenbeck Noise}     & $\mu = 0.0 \;\; \sigma = 0.3 \;\; \theta = 0.15$                  \\\midrule
			\textbf{Epsilon Decay Noise}          & \textbf{Start}: $0.9$, \textbf{End}: $0.2$, \textbf{Decay}: $200$ \\\midrule
			\textbf{Gamma ($\gamma$)}             & 0.99                                                              \\\midrule
			\textbf{Tau ($\tau$)}                 & $1 \times 10^{-3}$                                                \\\midrule
			\textbf{Observation}                  & \textbf{Buffer Size}: 2                                           \\
			                                      & \textbf{Image Size}: 64 $\times$ 64                               \\\midrule
			\textbf{Batch Size}                   & 64                                                                \\\midrule
			\textbf{Max Number of episode steps}  & 205                                                               \\\midrule
			\textbf{Replay Memory Size}           & 10000                                                             \\\midrule

			\textbf{\#Epoch per Episode}          & 250                                                               \\\midrule
			\textbf{Soft Target Update per Epoch} & 1                                                                 \\\midrule
			\textbf{Test Phase}                   & \textbf{Test frequency}: every 5000 epochs                        \\
			                                      & \textbf{Test episodes}: 10                                        \\
			\bottomrule
		\end{tabular}}
\end{table}

\begin{equation}
	\label{eq:epsilon_decay}
	\epsilon = \epsilon_{\text{start}} - (\epsilon_{\text{start}} -\epsilon_{\text{end}})\min(1.0, \frac{e}{\epsilon_{\text{decay}}})
\end{equation}

\FloatBarrier

\subsection{SAC Hyperparameters}

The hyper-parameters we exploited in this experiment are shown in \vref{table:sac_pendulum}.
\begin{table}[!h]
	\centering
	\caption{SAC Hyper-parameter setup for Pendulum-v0 environment}
	\label{table:sac_pendulum}
	\resizebox{0.9\textwidth}{!}{
		\begin{tabular}{@{}lllll@{}}
			\toprule
			\textbf{Hyper-parameters}             & \textbf{Value}                                         \\
			\midrule
			\textbf{Policy Network}               & \textbf{Learning Rate}: $3 \times 10^{-4}$             \\
			                                      & \textbf{Type}: Gaussian Policy                         \\
			                                      & \textbf{Architecture}                                  \\
			                                      & 3 CONV Layer $3\times 3\times 16$, stride 2, padding 0 \\
			                                      & 2 FC Layer with hidden size = 256                      \\
			                                      & 1 Output value                                         \\\midrule
			\textbf{Q Network}                    & \textbf{Learning Rate}: $3 \times 10^{-4}$             \\
			                                      & \textbf{Architecture}                                  \\
			                                      & 3 CONV Layer $3\times 3\times 16$, stride 2, padding 0 \\
			                                      & 2 FC Layer with hidden size = 256                      \\
			                                      & 1 Output value                                         \\\midrule
			\textbf{Gamma ($\gamma$)}             & 0.99                                                   \\\midrule
			\textbf{Tau ($\tau$)}                 & $5 \times 10^{-3}$                                     \\\midrule
			\textbf{Entropy Autotune}             & Enabled                                                \\\midrule
			\textbf{Observation}                  & \textbf{Buffer Size}: 2                                \\
			                                      & \textbf{Image Size}: 64 $\times$ 64                    \\\midrule
			\textbf{Batch Size}                   & 64                                                     \\\midrule
			\textbf{Max Number of episode steps}  & 200                                                    \\\midrule
			\textbf{Replay Memory Size}           & 10000                                                  \\\midrule

			\textbf{\#Epoch per Episode}          & 250                                                    \\\midrule
			\textbf{Soft Target Update per Epoch} & 1                                                      \\\midrule
			\textbf{Test Phase}                   & \textbf{Test frequency}: every 5000 epochs             \\
			                                      & \textbf{Test episodes}: 10                             \\
			\bottomrule
		\end{tabular}}
\end{table}

\FloatBarrier

\subsection{Comparative Analysis}

This section aims to present the most important plots we obtained from the experiments carried out on Pendulum-v0 environment.
The results obtained exploiting SAC algorithm are presented in \vref{fig:sac_pendulum_reward,fig:sac_pendulum_test_reward}, while the ones gathered with DDPG algorithm are presented in \vref{fig:ddpg_pendulum_reward,fig:ddpg_pendulum_test_reward}.

\Vref{fig:ddpg_pendulum_reward,fig:sac_pendulum_reward} shows the result of the training phase.
These plots has the number of episodes in the abscissa and the reward obtained in the ordinate.


The results of these graphs are unstable both in the average value and in margins sizes.
This phenomenon is mainly caused by the noise introduced during training: it allows the agent to explore the space in the environment without focusing on the current best action but trying to explore entirely and randomly the totality of environment space.
In the first case, the DDPG noise is given by the Ornstein Uhnlebeck process noise, while SAC exploits a Gaussian Policy by sampling a random action from the current distribution given by the network.
Therefore, SAC algorithms exploit the \textit{entropy autotune} presented by its authors: its main aim is to reduce the impact of hyper-parameters by automatically tuning the $\alpha$ temperature parameter.
This lead to a more straightforward setup of the experiment without requiring manual optimal temperature setup, which is non-trivial needs to be tuned for each task.

It is possible to investigate about these two trends in \vref{fig:ddpg_noise,fig:sac_temperature}.
We exploited \vref{eq:epsilon_decay} to manipulate the importance of the noise through the whole set of episode for each run.
The contribution of the noise decreases directly proportional to the number of episodes completed to enhance exploration in the initial part of the experiment.
On the other side, the auto-tuning approach of SAC influence the way the reward is calculated and used to train the network. The objective is to give more reward to actions that has an higher entropy, that is more unpredictable. This approach is motivated by the fact that an unpredictable situation can bring more information to the learning process than a more predictable one.
In D

It is noticeable that SAC results seem more valuable than DDPG ones: the SAC agent manages to touch the zero value after about 30 episodes and reaches a sort of asymptote between 100 and 200 after about 60 episodes.
On the other hand, the DDPG agent obtained a worse performance by touching the zero value after about 100 episodes and the corresponding values of SAC asymptote only in the last part of the experiment.
This fact is even more remarkable if we analyze the average of the previous 100 episodes carried out in training as shown in \vref{fig:ddpg_pendulum_last100,fig:sac_pendulum_last100}.

A criticism that can be made to this analysis is represented by the different amounts of noise present in different episodes.
If that were the case, the testing phase of the DDPG algorithm, where any noise is removed, should produce better performance or, at least, comparable to those of SAC.
However, analysing the result of the test phase shown in \vref{fig:ddpg_pendulum_test_reward,fig:sac_pendulum_test_reward}, it is clear that the different trends described above remain unchanged.
As mentioned before in this work, we decided to start a test phase every specified amount of epochs correctly executed.
For this reason, these plots has the number of epochs in the abscissa and the average reward obtained from 10 testing episode in the ordinate.
Even in this case, the outstanding performance of SAC is not achieved by the DDPG one even in the most testing phase, the most important.

%%%%%%%%%%%%%%%%%%%%%%
%
% REWARD PLOTS
%
%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[!h]
	\centering
	\begin{tikzpicture}
		\begin{axis}[axis on top,
				xmin=1,
				xmax=205,
				ymax=0,
				width=\textwidth*0.9,
				height=\textheight*0.4,
				set layers=standard,
				cycle list name=train,
				grid=both,
				grid style={solid,gray!30!white},
				% axis lines=middle,
				xlabel=Episode,
				ylabel style={align=center}, ylabel=Reward Value,
				%legend style={at={(0.99,0.3)},anchor=east},
				legend pos=south east,
				% extra y ticks = {90},
				% 	extra y tick style={grid=major, grid style={solid,green},y tick label style={
				% 		/pgf/number format/.cd,precision=10
				%}},
				% x label style={at={(axis description cs:0.5,0)},anchor=north},
				%y label style={at={(axis description cs:-0.1,.5)},rotate=90,anchor=south}
			]

			\addplot table[x=Step,y=Mean, col sep=comma] {plots/pendulum/ddpg_pendulum_reward.csv};
			%\addlegendentry{Mean Reward of last 100 episode};

			\addplot [name path=upper,draw=none, forget plot] table[x=Step,y expr=\thisrow{Max}, col sep=comma] {plots/pendulum/ddpg_pendulum_reward.csv};

			\addplot [name path=lower,draw=none, forget plot] table[x=Step,y expr=\thisrow{Min}, col sep=comma] {plots/pendulum/ddpg_pendulum_reward.csv};

			\addplot [fill=train_color_3] fill between[of=upper and lower];

			\addplot [name path=upper1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Mean}+\thisrow{StDev}, col sep=comma] {plots/pendulum/ddpg_pendulum_reward.csv};

			\addplot [name path=lower1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Mean}-\thisrow{StDev}, col sep=comma] {plots/pendulum/ddpg_pendulum_reward.csv};

			\addplot [fill=train_color_2] fill between[of=upper1 and lower1];

			\addlegendentry{Mean $\mu$};
			\addlegendentry{Area $[min, max]$};
			\addlegendentry{Area $[\mu-\sigma, \mu+\sigma]$};

		\end{axis}
	\end{tikzpicture}
	\caption[DDPG Pendulum-v0 Reward Plot]{DDPG Pendulum-v0 Reward Plot. The graph reports mean, standard deviation range and min-max range of the reward of each episode over 10 runs with different seeds.}
	\label{fig:ddpg_pendulum_reward}
\end{figure}

\begin{figure}[!h]
	\centering
	\begin{tikzpicture}
		\begin{axis}[axis on top,
				xmin=1,
				xmax=205,
				ymax=0,
				width=\textwidth*0.9,
				height=\textheight*0.4,
				set layers=standard,
				cycle list name=train,
				grid=both,
				grid style={solid,gray!30!white},
				% axis lines=middle,
				xlabel=Episode,
				ylabel style={align=center}, ylabel=Reward Value,
				%legend style={at={(0.99,0.3)},anchor=east},
				legend pos=south east,
				% extra y ticks = {90},
				% 	extra y tick style={grid=major, grid style={solid,green},y tick label style={
				% 		/pgf/number format/.cd,precision=10
				%}},
				% x label style={at={(axis description cs:0.5,0)},anchor=north},
				%y label style={at={(axis description cs:-0.1,.5)},rotate=90,anchor=south}
			]

			\addplot table[x=Step,y=Mean, col sep=comma] {plots/pendulum/sac_pendulum_reward.csv};
			%\addlegendentry{Mean Reward of last 100 episode};

			\addplot [name path=upper,draw=none, forget plot] table[x=Step,y expr=\thisrow{Max}, col sep=comma] {plots/pendulum/sac_pendulum_reward.csv};

			\addplot [name path=lower,draw=none, forget plot] table[x=Step,y expr=\thisrow{Min}, col sep=comma] {plots/pendulum/sac_pendulum_reward.csv};

			\addplot [fill=train_color_3] fill between[of=upper and lower];

			\addplot [name path=upper1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Mean}+\thisrow{StDev}, col sep=comma] {plots/pendulum/sac_pendulum_reward.csv};

			\addplot [name path=lower1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Mean}-\thisrow{StDev}, col sep=comma] {plots/pendulum/sac_pendulum_reward.csv};

			\addplot [fill=train_color_2] fill between[of=upper1 and lower1];

			\addlegendentry{Mean $\mu$};
			\addlegendentry{Area $[min, max]$};
			\addlegendentry{Area $[\mu-\sigma, \mu+\sigma]$};

		\end{axis}
	\end{tikzpicture}
	\caption[SAC Pendulum-v0 Reward Plot]{SAC Pendulum-v0 Reward Plot. The graph reports mean, standard deviation range and min-max range of the reward of each episode over 10 runs with different seeds.}
	\label{fig:sac_pendulum_reward}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%
%
% TEST PLOTS
%
%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[!h]
	\centering
	\begin{tikzpicture}
		\begin{axis}[axis on top,
				width=\textwidth*0.9,
				height=\textheight*0.38,
				xmin=5000,
				xmax=50000,
				ymax=0,
				set layers=standard,
				cycle list name=test,
				grid=both,
				grid style={solid,gray!30!white},
				% axis lines=middle,
				xlabel=Epochs,
				ylabel style={align=center}, ylabel=Average Reward Value,
				%legend style={at={(0.99,0.3)},anchor=east},
				legend pos=south east,
				% extra y ticks = {90},
				% 	extra y tick style={grid=major, grid style={solid,green},y tick label style={
				% 		/pgf/number format/.cd,precision=10
				%}},
				% x label style={at={(axis description cs:0.5,0)},anchor=north},
				%y label style={at={(axis description cs:-0.1,.5)},rotate=90,anchor=south}
			]

			\addplot table[x=Step,y=Mean, col sep=comma] {plots/pendulum/ddpg_pendulum_test.csv};
			%\addlegendentry{Mean Reward of last 100 episode};

			\addplot [name path=upper,draw=none, forget plot] table[x=Step,y expr=\thisrow{Max}, col sep=comma] {plots/pendulum/ddpg_pendulum_test.csv};

			\addplot [name path=lower,draw=none, forget plot] table[x=Step,y expr=\thisrow{Min}, col sep=comma] {plots/pendulum/ddpg_pendulum_test.csv};

			\addplot [fill=test_color_3] fill between[of=upper and lower];

			\addplot [name path=upper1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Mean}+\thisrow{StDev}, col sep=comma] {plots/pendulum/ddpg_pendulum_test.csv};

			\addplot [name path=lower1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Mean}-\thisrow{StDev}, col sep=comma] {plots/pendulum/ddpg_pendulum_test.csv};

			\addplot [fill=test_color_2] fill between[of=upper1 and lower1];

			\addlegendentry{Mean $\mu$};
			\addlegendentry{Area $[min, max]$};
			\addlegendentry{Area $[\mu-\sigma, \mu+\sigma]$};

		\end{axis}
	\end{tikzpicture}
	\caption[DDPG Pendulum-v0 Test Average Reward Plot]{DDPG Pendulum-v0 Test Average Reward Plot. The graph reports mean, standard deviation range and min-max range of the average reward obtained from 10 test episodes every 5000 epochs. They are calculated on 10 runs with different seeds.
	}
	\label{fig:ddpg_pendulum_test_reward}
\end{figure}

\begin{figure}[!h]
	\centering
	\begin{tikzpicture}
		\begin{axis}[axis on top,
				width=\textwidth*0.9,
				height=\textheight*0.38,
				xmin=5000,
				xmax=50000,
				ymax=0,
				set layers=standard,
				cycle list name=test,
				grid=both,
				grid style={solid,gray!30!white},
				% axis lines=middle,
				xlabel=Epochs,
				ylabel style={align=center}, ylabel=Average Reward Value,
				%legend style={at={(0.99,0.3)},anchor=east},
				legend pos=south east,
				% extra y ticks = {90},
				% 	extra y tick style={grid=major, grid style={solid,green},y tick label style={
				% 		/pgf/number format/.cd,precision=10
				%}},
				% x label style={at={(axis description cs:0.5,0)},anchor=north},
				%y label style={at={(axis description cs:-0.1,.5)},rotate=90,anchor=south}
			]

			\addplot table[x=Step,y=Mean, col sep=comma] {plots/pendulum/sac_pendulum_test.csv};
			%\addlegendentry{Mean Reward of last 100 episode};

			\addplot [name path=upper,draw=none, forget plot] table[x=Step,y expr=\thisrow{Max}, col sep=comma] {plots/pendulum/sac_pendulum_test.csv};

			\addplot [name path=lower,draw=none, forget plot] table[x=Step,y expr=\thisrow{Min}, col sep=comma] {plots/pendulum/sac_pendulum_test.csv};

			\addplot [fill=test_color_3] fill between[of=upper and lower];

			\addplot [name path=upper1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Mean}+\thisrow{StDev}, col sep=comma] {plots/pendulum/sac_pendulum_test.csv};

			\addplot [name path=lower1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Mean}-\thisrow{StDev}, col sep=comma] {plots/pendulum/sac_pendulum_test.csv};

			\addplot [fill=test_color_2] fill between[of=upper1 and lower1];

			\addlegendentry{Mean $\mu$};
			\addlegendentry{Area $[min, max]$};
			\addlegendentry{Area $[\mu-\sigma, \mu+\sigma]$};

		\end{axis}
	\end{tikzpicture}
	\caption[SAC Pendulum-v0 Test Average Reward Plot]{SAC Pendulum-v0 Test Average Reward Plot. The graph reports mean, standard deviation range and min-max range of the average reward obtained from 10 test episodes every 5000 epochs. They are calculated on 10 runs with different seeds.}
	\label{fig:sac_pendulum_test_reward}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%
%
% LAST 100 STEPS MEAN
%
%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[!h]
	\centering
	\begin{tikzpicture}
		\begin{axis}[axis on top,
				width=\textwidth*0.9,
				height=\textheight*0.4,
				xmin=1,
				xmax=205,
				ymax=0,
				set layers=standard,
				cycle list name=train,
				grid=both,
				grid style={solid,gray!30!white},
				% axis lines=middle,
				xlabel=Epochs,
				ylabel style={align=center}, ylabel=Average Reward Value,
				%legend style={at={(0.99,0.3)},anchor=east},
				legend pos=south east,
				% extra y ticks = {90},
				% 	extra y tick style={grid=major, grid style={solid,green},y tick label style={
				% 		/pgf/number format/.cd,precision=10
				%}},
				% x label style={at={(axis description cs:0.5,0)},anchor=north},
				%y label style={at={(axis description cs:-0.1,.5)},rotate=90,anchor=south}
			]

			\addplot table[x=Step,y=Mean, col sep=comma] {plots/pendulum/ddpg_pendulum_last100.csv};
			%\addlegendentry{Mean Reward of last 100 episode};

			\addplot [name path=upper,draw=none, forget plot] table[x=Step,y expr=\thisrow{Max}, col sep=comma] {plots/pendulum/ddpg_pendulum_last100.csv};

			\addplot [name path=lower,draw=none, forget plot] table[x=Step,y expr=\thisrow{Min}, col sep=comma] {plots/pendulum/ddpg_pendulum_last100.csv};

			\addplot [fill=train_color_3] fill between[of=upper and lower];

			\addplot [name path=upper1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Mean}+\thisrow{StDev}, col sep=comma] {plots/pendulum/ddpg_pendulum_last100.csv};

			\addplot [name path=lower1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Mean}-\thisrow{StDev}, col sep=comma] {plots/pendulum/ddpg_pendulum_last100.csv};

			\addplot [fill=train_color_2] fill between[of=upper1 and lower1];

			\addlegendentry{Mean $\mu$};
			\addlegendentry{Area $[min, max]$};
			\addlegendentry{Area $[\mu-\sigma, \mu+\sigma]$};

		\end{axis}
	\end{tikzpicture}
	\caption[DDPG Pendulum-v0 Last 100 Episode Average Reward Plot]{DDPG Pendulum-v0 Last 100 Episode Average Reward Plot. The graph reports mean, standard deviation range and min-max range of the last 100 episode average reward for each episode over 10 runs with different seeds.}
	\label{fig:ddpg_pendulum_last100}
\end{figure}

\begin{figure}[!h]
	\centering
	\begin{tikzpicture}
		\begin{axis}[axis on top,
				width=\textwidth*0.9,
				height=\textheight*0.4,
				xmin=1,
				xmax=205,
				ymax=0,
				set layers=standard,
				cycle list name=train,
				grid=both,
				grid style={solid,gray!30!white},
				% axis lines=middle,
				xlabel=Epochs,
				ylabel style={align=center}, ylabel=Average Reward Value,
				%legend style={at={(0.99,0.3)},anchor=east},
				legend pos=south east,
				% extra y ticks = {90},
				% 	extra y tick style={grid=major, grid style={solid,green},y tick label style={
				% 		/pgf/number format/.cd,precision=10
				%}},
				% x label style={at={(axis description cs:0.5,0)},anchor=north},
				%y label style={at={(axis description cs:-0.1,.5)},rotate=90,anchor=south}
			]

			\addplot table[x=Step,y=Mean, col sep=comma] {plots/pendulum/sac_pendulum_last100.csv};
			%\addlegendentry{Mean Reward of last 100 episode};

			\addplot [name path=upper,draw=none, forget plot] table[x=Step,y expr=\thisrow{Max}, col sep=comma] {plots/pendulum/sac_pendulum_last100.csv};

			\addplot [name path=lower,draw=none, forget plot] table[x=Step,y expr=\thisrow{Min}, col sep=comma] {plots/pendulum/sac_pendulum_last100.csv};

			\addplot [fill=train_color_3] fill between[of=upper and lower];

			\addplot [name path=upper1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Mean}+\thisrow{StDev}, col sep=comma] {plots/pendulum/sac_pendulum_last100.csv};

			\addplot [name path=lower1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Mean}-\thisrow{StDev}, col sep=comma] {plots/pendulum/sac_pendulum_last100.csv};

			\addplot [fill=train_color_2] fill between[of=upper1 and lower1];

			\addlegendentry{Mean $\mu$};
			\addlegendentry{Area $[min, max]$};
			\addlegendentry{Area $[\mu-\sigma, \mu+\sigma]$};

		\end{axis}
	\end{tikzpicture}
	\caption[SAC Pendulum-v0 Last 100 Episode Average Reward Plot]{SAC Pendulum-v0 Last 100 Episode Average Reward Plot. The graph reports mean, standard deviation range and min-max range of the last 100 episode average reward for each episode over 10 runs with different seeds.}
	\label{fig:sac_pendulum_last100}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%
%
% NOISE PLOT
%
%%%%%%%%%%%%%%%%%%%%%%

\begin{figure}[!h]
	\centering
	\begin{tikzpicture}
		\begin{axis}[axis on top,
				width=\textwidth*0.9,
				height=\textheight*0.4,
				xmin=1,
				xmax=205,
				ymax=1,
				ymin=0,
				set layers=standard,
				cycle list name=train,
				grid=both,
				grid style={solid,gray!30!white},
				% axis lines=middle,
				xlabel=Epochs,
				ylabel style={align=center}, ylabel=Average Reward Value,
				%legend style={at={(0.99,0.3)},anchor=east},
				legend pos=north east,
				% extra y ticks = {90},
				% 	extra y tick style={grid=major, grid style={solid,green},y tick label style={
				% 		/pgf/number format/.cd,precision=10
				%}},
				% x label style={at={(axis description cs:0.5,0)},anchor=north},
				%y label style={at={(axis description cs:-0.1,.5)},rotate=90,anchor=south}
			]

			\addplot table[x=Step,y=Mean, col sep=comma] {plots/pendulum/ddpg_pendulum_noise.csv};
			%\addlegendentry{Mean Reward of last 100 episode};

			% \addplot [name path=upper,draw=none, forget plot] table[x=Step,y expr=\thisrow{Max}, col sep=comma] {plots/pendulum/ddpg_pendulum_noise.csv};

			% \addplot [name path=lower,draw=none, forget plot] table[x=Step,y expr=\thisrow{Min}, col sep=comma] {plots/pendulum/ddpg_pendulum_noise.csv};

			% \addplot [fill=train_color_3] fill between[of=upper and lower];

			% \addplot [name path=upper1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Mean}+\thisrow{StDev}, col sep=comma] {plots/pendulum/ddpg_pendulum_noise.csv};

			% \addplot [name path=lower1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Mean}-\thisrow{StDev}, col sep=comma] {plots/pendulum/ddpg_pendulum_noise.csv};

			% \addplot [fill=train_color_2] fill between[of=upper1 and lower1];

			\addlegendentry{Epsilon value $\epsilon$};
			\addlegendentry{Area $[min, max]$};
			\addlegendentry{Area $[\mu-\sigma, \mu+\sigma]$};

		\end{axis}
	\end{tikzpicture}
	\caption[DDPG Pendulum-v0 Noise Epsilon Decay]{DDPG Pendulum-v0 Noise Epsilon Decay. The graph shows the trend of the noise epsilon decay applied to the Ornstein Uhlenbeck noise in DDPG.}
	\label{fig:ddpg_noise}
\end{figure}

\begin{figure}[!h]
	\centering
	\begin{tikzpicture}
		\begin{axis}[axis on top,
				width=\textwidth*0.9,
				height=\textheight*0.4,
				xmin=1,
				xmax=50000,
				ymax=1,
				ymin=0,
				set layers=standard,
				cycle list name=train,
				grid=both,
				grid style={solid,gray!30!white},
				% axis lines=middle,
				xlabel=Epochs,
				ylabel style={align=center}, ylabel=Temperature Value ($\alpha$),
				%legend style={at={(0.99,0.3)},anchor=east},
				legend pos=north east,
				% extra y ticks = {90},
				% 	extra y tick style={grid=major, grid style={solid,green},y tick label style={
				% 		/pgf/number format/.cd,precision=10
				%}},
				% x label style={at={(axis description cs:0.5,0)},anchor=north},
				%y label style={at={(axis description cs:-0.1,.5)},rotate=90,anchor=south}
			]

			\addplot table[x=Step,y=Mean, col sep=comma] {plots/pendulum/sac_pendulum_temperature.csv};
			%\addlegendentry{Mean Reward of last 100 episode};

			\addplot [name path=upper,draw=none, forget plot] table[x=Step,y expr=\thisrow{Max}, col sep=comma] {plots/pendulum/sac_pendulum_temperature.csv};

			\addplot [name path=lower,draw=none, forget plot] table[x=Step,y expr=\thisrow{Min}, col sep=comma] {plots/pendulum/sac_pendulum_temperature.csv};

			\addplot [fill=train_color_3] fill between[of=upper and lower];

			\addplot [name path=upper1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Mean}+\thisrow{StDev}, col sep=comma] {plots/pendulum/sac_pendulum_temperature.csv};

			\addplot [name path=lower1,draw=none, forget plot] table[x=Step,y expr=\thisrow{Mean}-\thisrow{StDev}, col sep=comma] {plots/pendulum/sac_pendulum_temperature.csv};

			\addplot [fill=train_color_2] fill between[of=upper1 and lower1];

			\addlegendentry{Mean $\mu$};
			\addlegendentry{Area $[min, max]$};
			\addlegendentry{Area $[\mu-\sigma, \mu+\sigma]$};

		\end{axis}
	\end{tikzpicture}
	\caption[SAC Pendulum-v0 auto-tuning temperature]{SAC Pendulum-v0 auto-tuning temperature. The graph shows the trend of the temperature parameter learned through the auto-tune process proposed by SAC authors.}
	\label{fig:sac_temperature}
\end{figure}

\FloatBarrier

\section{CozmoEnv-v0 Experiments} \label{sec:cozmo-exp}

\subsection{DDPG Hyperparameters}

\subsection{SAC Hyperparameters}

\subsection{Comparative Analysis}

\subsection{Results}



\todomacaluso{
	\begin{itemize}
		\item Introduction: arguments of the chapter and overview of final results
		\item Experimental Methodology
		      \begin{itemize}
			      \item Preliminaries: experiments with Pendulum-v0
			      \item Real Life experiments with Cozmo
			      \item Hyper-Parameters discussion and motivation
			      \item Algorithms applied and modifications with pseudo-code
		      \end{itemize}
		\item Experiments with Pendulum-v0
		      \begin{itemize}
			      \item Comparative analysis between results obtained with DDPG and SAC
			      \item Results
		      \end{itemize}
		\item Experiments with Cozmo
		      \begin{itemize}
			      \item Comparative analysis between results obtained with DDPG and SAC
			      \item Results
		      \end{itemize}
	\end{itemize}
}
