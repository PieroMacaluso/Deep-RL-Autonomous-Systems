\appendix

\chapter{Reinforcement Learning}

\section{Bellman Equation} \label{appendix:bellmaneq}

\todomacaluso{Check correctness and completeness}

The value function is decomposable in the immediate reward $r_t$ and the discounted state value of the next state. It is possible to obtain the result in \vref{eq:decompvalue} by writing expectations explicitly.

\begin{align}\label{eq:decompvalue}
\begin{split}
V^\pi(s) &= \mathbb{E}[g_t | s_t = s] \\
&= \mathbb{E}[r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \dots | s_t = s] \\
&= \mathbb{E}[r_{t+1} + \gamma g_{t+1} | s_t = s] \\
&= \sum_{a \in \mathcal{A}}\pi(a|s)\sum_{s' \in \mathcal{S}, r \in \mathcal{R}}P(s', r | s, a)\big[r + \gamma\mathbb{E}[g_{t+1}| s_{t+1} = s']\big]\\
&= \sum_{a \in \mathcal{A}}\pi(a|s)\sum_{s' \in \mathcal{S}, r \in \mathcal{R}}P(s', r | s, a)\big[r + \gamma V^\pi(s')\big]
\end{split}
\end{align}

This equation expresses the relationship between the value of a state and the values of its successor states. It is further possible to derive the Bellman Equation for Action-Value function using the same procedure described above.

The resulting formulas are shown in \vref{eq:bellman}.

Furthermore, it is possible to obtain the Bellman Equation solution in \vref{eq:bellmanstate} working with matrix notation.
\begin{align} \label{eq:bellmanstate}
\begin{split}
V^\pi &= \mathcal{R}^\pi + \gamma \mathcal{P}^\pi V^\pi \\
(I - \gamma\mathcal{P}^\pi)V^\pi &= \mathcal{R}^\pi \\
V^\pi &= (I - \gamma\mathcal{P}^\pi)^{-1}\mathcal{R}^\pi
\end{split}
\end{align}

\section{Dynamic programming} \label{policyimprovement}

\subsubsection{Policy iteration algorithm}

\begin{algorithm}[!htp]
	\SetAlgoLined
	\DontPrintSemicolon
	\LinesNumbered
	\KwIn{$\pi$ the policy to be evaluated; a small threshold $\theta$ which defines the accuracy of the estimation\;}
	Initialise $V(s) \; \forall s \in \mathcal{S}$ arbitrarily, except that $V(terminal) = 0$\;
	$is\_policy\_stable \leftarrow true$\;
	\Repeat{$\neg \; is\_policy\_stable$}{
		\tcc{Policy Evaluation}
		\Repeat{$\Delta < \theta$}{
			$\Delta \leftarrow 0$\;
			\For{each $s \in \mathcal{S}$}{
				v $\leftarrow$ $V(s)$\;
				$V(s) \leftarrow \sum_{a \in \mathcal{A}} \pi(a|s) \sum_{s' \in \mathcal{S}, r \in \mathcal{R}} P(s', r|s,a) \big[r+\gamma V(s')\big]$\;
				$\Delta \leftarrow \max(\Delta, |v - V(s)|)$	
			}
		}
		$V_\pi \leftarrow V(s)$\;
		\tcc{Policy Improvement}
		
		\While{true}{
			\For{each $s \in \mathcal{S}$}{
				$old\_action \leftarrow \pi(s)$\;
				$\pi(s) \leftarrow \underset{a}{\arg\max\,} \sum_{s' \in \mathcal{S}, r \in \mathcal{R}}P(s',r |s, a)\bigg[r+\gamma V_\pi(s')\bigg]$\;
				\If{$old\_action \neq \pi(s)$}{$is\_policy\_stable \leftarrow false$}
			}
		}
	}
	\KwOut{$V^*$ and $\pi^*$}
	\caption{Policy Iteration for estimating $\pi \sim \pi^*$}
	\label{policy_evaluation}
\end{algorithm}

\subsubsection{Policy improvement theorem}

Let $\pi$ and $\pi'$ be any pair of deterministic policy such that 
\begin{equation} \label{eq:impr0}
	Q_\pi(s, \pi'(s)) \ge V_\pi(s) \; \forall s \in S
\end{equation}
Then the policy $\pi'$ leads to
\begin{equation} \label{eq:impr1}
V_\pi'(s) \ge V_\pi(s)
\end{equation}


Therefore, the presence of strict inequality in \vref{eq:impr0} for a state leads to a strict inequality of \vref{eq:impr1}.

The proof of this theorem is shown in \vref{eq:policyimprovement}.
\begin{align}\label{eq:policyimprovement}
\begin{split}
V_\pi(s) &\le Q_\pi(s, \pi'(s))\\
		&= \mathbb{E}[r_{t+1} + \gamma V_\pi(s_{t+1})| s_t = s, a_t = \pi'(s)]\\
		&= \mathbb{E}_{\pi'}[r_{t+1} + \gamma V_\pi(s_{t+1})| s_t = s]\\
		&\le \mathbb{E}_{\pi'}[r_{t+1} + \gamma Q_\pi(s_{t+1}, \pi'(s_{t+1}))| s_t = s] \; \; \text{(by \ref{eq:impr0})}\\
		&= \mathbb{E}_{\pi'}[r_{t+1} + \gamma \mathbb{E}_{\pi'}[r_{t+2}+ \gamma V_\pi(s_{t+2})| s_{t+1}, a_{t+1} = \pi'(s_{t+1})]| s_t = s]\\
		&= \mathbb{E}_{\pi'}[r_{t+1} + \gamma r_{t+2} + \gamma^2 V_\pi(s_{t+2})| s_t = s]\\
		&\le \mathbb{E}_{\pi'}[r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \gamma^3 V_\pi(s_{t+3})| s_t = s]\\
		&\vdots\\
		&\le \mathbb{E}_{\pi'}[r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \gamma^3 r_{t+4} + \dots| s_t = s]\\
		&= v_{\pi'}(s)
\end{split}
\end{align}

\subsubsection{Value iteration algorithm}

\begin{algorithm}[!htp]
	\SetAlgoLined
	\DontPrintSemicolon
	\LinesNumbered
	\KwIn{A small threshold $\theta$ which defines the accuracy of the estimation\;}
	Initialise $V(s) \; \forall s \in \mathcal{S}$ arbitrarily, except that $V(terminal) = 0$\;
	\Repeat{$\Delta < \theta$}{
		$\Delta \leftarrow 0$\;
		\For{each $s \in \mathcal{S}$}{
			v $\leftarrow$ $V(s)$\;
			$V(s) \leftarrow \max_{a}\sum_{s' \in \mathcal{S}, r \in \mathcal{R}} P(s', r|s,a) \big[r+\gamma V(s')\big]$\;
			$\Delta \leftarrow \max(\Delta, |v - V(s)|)$	
		}
	}
	Output a deterministic policy, $\pi \sim \pi^*$, such that \[
	\pi(s) = \underset{a}{\arg\max\,} \sum_{s' \in \mathcal{S}, r \in \mathcal{R}} P(s', r|s,a) \big[r+\gamma V(s')\big]\]\\
	\KwOut{$V^*$ and $\pi^*$}
	\caption{Value Iteration, for estimating $\pi \sim \pi^*$}
	\label{value_iteration}
\end{algorithm}


